{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the node-node-label graph : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"BlogCatalog\"\n",
    "edges_file = data_name + \"/edges.csv\"\n",
    "node_label_file = data_name + \"/group-edges.csv\"\n",
    "label_occ_file = data_name + \"/label_co-occurences.csv\"\n",
    "nnlg_file = data_name + \"/edges_node_node_label.csv\"\n",
    "llng_file = data_name + \"/edges_label_label_node.csv\"\n",
    "label_raw, nodes = [], []\n",
    "with open(node_label_file) as file_to_read: \n",
    "    while True:\n",
    "        lines = file_to_read.readline()\n",
    "        if not lines:\n",
    "            break \n",
    "        node, label = lines.split(\",\")\n",
    "        label_raw.append(int(label))\n",
    "        nodes.append(int(node))\n",
    "\n",
    "label_raw = np.array(label_raw)\n",
    "nodes = np.array(nodes)\n",
    "unique_nodes = np.unique(nodes)\n",
    "labels = np.zeros((unique_nodes.shape[0], 39))\n",
    "for l in range(1, 40, 1):\n",
    "    indices = np.argwhere(label_raw == l).reshape(-1)\n",
    "    n_l = nodes[indices]\n",
    "    for n in n_l:\n",
    "        labels[n-1][l-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_nodes = label_raw + unique_nodes.shape[0] \n",
    "n_n_l_nodes = np.concatenate((unique_nodes, np.unique(label_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list())\n",
    "df.to_csv(nnlg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(nnlg_file, \"r+\")\n",
    "file_to_read = open(edges_file, \"r\")\n",
    "f.writelines(file_to_read.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.dstack((nodes, label_nodes)).reshape(label_nodes.shape[0],2)\n",
    "e = [\"\\n\"] + [\",\".join(item)+\"\\n\" for item in a.astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.writelines(e)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnlg_file = \"BlogCatalog/edges_node_node_label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnl_graph = nx.read_edgelist(nnlg_file, delimiter = \",\", nodetype = int)\n",
    "E = nx.adjacency_matrix(nnl_graph, nodelist = n_n_l_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = open(edges_file, \"rb\")\n",
    "G = nx.read_edgelist(main_graph, delimiter = \",\", nodetype = int)\n",
    "A = nx.adjacency_matrix(G, nodelist = unique_nodes)\n",
    "A = sp.coo_matrix(A.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature matrix of the common nodes\n",
    "X = sp.csr_matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tilde = normalize(A + sp.eye(A.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the label-label-node graph : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "list_edges = []\n",
    "for k in range(labels.shape[0]):\n",
    "    indices = np.argwhere(labels[k] == 1).reshape(-1)\n",
    "    if indices.shape[0]>1:\n",
    "        for subset in itertools.combinations(indices, 2): \n",
    "            if (list(subset) not in list_edges) or ([subset[1], subset[0]] not in list_edges):\n",
    "                list_edges.append([subset[0]+labels.shape[0], subset[1]+labels.shape[0]])# check if the common nodes should be before or after the label nodes\n",
    "                edges.append(str(subset[0]+1 + labels.shape[0]) + \",\" + str(subset[1] +1 + labels.shape[0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list())\n",
    "df.to_csv(label_occ_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = open(label_occ_file, \"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file.writelines(edges)\n",
    "label_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label_ID = np.arange(1,40) + labels.shape[0]\n",
    "label_file = open(label_occ_file, \"rb\")\n",
    "label_graph = nx.read_edgelist(label_file, delimiter = \",\", nodetype = int)\n",
    "C = nx.adjacency_matrix(label_graph, nodelist = unique_label_ID)\n",
    "label_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_tilde = normalize(C + sp.eye(C.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature matrix of the label nodes\n",
    "Y = X[:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ind = label_raw + labels.shape[0]\n",
    "a_1 = np.dstack((labels_ind,nodes)).reshape(labels_ind.shape[0],2)\n",
    "e_1 = [\"\\n\"] + [\",\".join(item)+\"\\n\" for item in a_1.astype(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(label_occ_file, \"r+\")\n",
    "file.writelines(e_1)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = open(label_occ_file, \"rb\")\n",
    "l_l_n_nodes = np.concatenate((np.unique(nodes),np.unique(labels_ind)))\n",
    "lln_graph = nx.read_edgelist(f_1, delimiter = \",\", nodetype = int)\n",
    "F = nx.adjacency_matrix(lln_graph, nodelist = l_l_n_nodes)\n",
    "F = sp.coo_matrix(F.todense())\n",
    "f_1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_star = sp.vstack((X,Y))\n",
    "Y_star = sp.vstack((Y,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = normalize(E + sp.eye(E.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_tilde = E[:len(unique_nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = normalize(F + sp.eye(F.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_tilde = F[len(unique_nodes):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10312x10312 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 667966 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39x10312 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 3408 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10351x10312 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 671374 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_star "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10351x10312 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 671374 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39x10351 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14515 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10312x10351 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 692754 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39x39 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1269 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for the train/validation/test \n",
    "indices = np.arange(A.shape[0]).astype('int32')\n",
    "idx_train = indices[:A.shape[0] // 3]\n",
    "idx_val = indices[A.shape[0] // 3: (2 * A.shape[0]) // 3]\n",
    "idx_test = indices[(2 * A.shape[0]) // 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to tensors for the training \n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)\n",
    "X_star = torch.FloatTensor(np.array(X_star.todense()))\n",
    "Y_star = torch.FloatTensor(np.array(Y_star.todense()))\n",
    "C_tilde = torch.FloatTensor(np.array(C_tilde.todense()))\n",
    "E_tilde = torch.FloatTensor(np.array(E_tilde.todense()))\n",
    "F_tilde = torch.FloatTensor(np.array(F_tilde.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_star = torch.FloatTensor(np.array(X_star.todense()))\n",
    "Y_star = torch.FloatTensor(np.array(Y_star.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tilde = torch.FloatTensor(np.array(A_tilde.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10351, 10312])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_star.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get down to the training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code.models import High_Layer, Low_Layer\n",
    "import time\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.02,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=400,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(output):\n",
    "    output[output > 0.5] = 1\n",
    "    output[output <= 0.5] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_high_layer(Y_star):\n",
    "    high_layer = High_Layer(nfeat = A_tilde.shape[0],\n",
    "                            nhid = args.hidden,\n",
    "                            nclass = C_tilde.shape[0],\n",
    "                            dropout = args.dropout)\n",
    "    optimizer_highLayer = optim.SGD(high_layer.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    high_layer.train()\n",
    "    optimizer_highLayer.zero_grad()\n",
    "    Y_new = high_layer(Y_star, F_tilde, C_tilde)\n",
    "    print(Y_new.shape)\n",
    "    # Calculate the train loss (Cross-Entropy)\n",
    "    truth = torch.LongTensor(np.arange(39))\n",
    "    loss = F.cross_entropy(Y_new, truth) \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    optimizer_highLayer.step()\n",
    "    \n",
    "    \n",
    "    return Y_new, loss, high_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0266, 0.0236, 0.0195,  ..., 0.0225, 0.0287, 0.0241],\n",
       "         [0.0264, 0.0235, 0.0195,  ..., 0.0226, 0.0286, 0.0241],\n",
       "         [0.0265, 0.0236, 0.0195,  ..., 0.0226, 0.0287, 0.0242],\n",
       "         ...,\n",
       "         [0.0269, 0.0236, 0.0193,  ..., 0.0224, 0.0283, 0.0243],\n",
       "         [0.0274, 0.0239, 0.0197,  ..., 0.0229, 0.0284, 0.0245],\n",
       "         [0.0239, 0.0212, 0.0201,  ..., 0.0234, 0.0307, 0.0268]],\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " tensor(3.6635, grad_fn=<NllLossBackward>))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_high_layer(Y_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sample_class(output, labels):\n",
    "    \"\"\" \n",
    "    output is of shape (N,C)\n",
    "    Labels is of shape (N,C)\n",
    "    Result : acc gives the accuracy computed according to the sample-class view\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    C = labels.shape[1]\n",
    "    corr = np.sum(np.equal(output, labels))\n",
    "    # corr is the number of equal elements between labels and output and thus the number of correctly classified \n",
    "    # labels for each sample \n",
    "    acc = corr/(N*C)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_low_layer(X_star):\n",
    "    lowï¼¿layer = Low_Layer(nfeat = A_tilde.shape[0],\n",
    "                           nhid = args.hidden,\n",
    "                           nclass = C_tilde.shape[0],\n",
    "                           dropout = args.dropout)\n",
    "    optimizer_lowLayer = optim.SGD(low_layer.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    low_layer.train()\n",
    "    optimizer_lowLayer.zero_grad()\n",
    "    X_new = low_layer(X_star, E_tilde, A_tilde)\n",
    "    # Calculate the train loss (Binary Cross Entropy)\n",
    "    loss_train = (1/39)*np.sum([F.binary_cross_entropy_with_logits(X_new[idx_train][:,i], labels[idx_train][:,i]) for i in range(C_tilde.shape[0])])\n",
    "    acc_train = accuracy_sample_class(threshold(X_new.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer_lowLayer.step()\n",
    "    \n",
    "    loss_val = (1/39)*np.sum([F.binary_cross_entropy_with_logits(X_new[idx_val][:,i], labels[idx_val][:,i]) for i in range(C_tilde.shape[0])])\n",
    "    \n",
    "    return X_new, loss_train, loss_val, acc_train, low_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4698, 0.5292, 0.5110,  ..., 0.5279, 0.5029, 0.5689],\n",
       "         [0.4700, 0.5306, 0.5535,  ..., 0.5165, 0.5172, 0.5570],\n",
       "         [0.4688, 0.5311, 0.4944,  ..., 0.5256, 0.4911, 0.5645],\n",
       "         ...,\n",
       "         [0.4738, 0.5341, 0.4856,  ..., 0.5129, 0.5272, 0.5972],\n",
       "         [0.5097, 0.5408, 0.4570,  ..., 0.5168, 0.5251, 0.6087],\n",
       "         [0.4262, 0.5542, 0.4676,  ..., 0.5228, 0.5925, 0.6217]],\n",
       "        grad_fn=<SigmoidBackward>),\n",
       " tensor(0.9659, grad_fn=<MulBackward0>),\n",
       " tensor(0.9663, grad_fn=<MulBackward0>),\n",
       " 0.4563908596495155,\n",
       " Low_Layer(\n",
       "   (gc1): GraphConvolution (10312 -> 400)\n",
       "   (gc2): GraphConvolution (400 -> 39)\n",
       " ))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_low_layer(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new, loss_train, loss_val = train_low_layer(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global train function for having a loop \n",
    "# epochs M N for\n",
    "def global_train(epochs, M, N, Y_star, X_star):\n",
    "    Y_star1 = Y_star\n",
    "    X_star1 = X_star\n",
    "    for i in range(epochs):\n",
    "        Y_new, loss_train_hl, high_layer = train_high_layer(Y_star1)\n",
    "        X_new, loss_train_ll, loss_val_ll, acc_train_ll, low_layer = train_low_layer(X_star1)\n",
    "        \n",
    "        if i%M:\n",
    "            X_star1 = np.concatenate((Y_new, X_new),axis = 1)\n",
    "            \n",
    "        if i%N:\n",
    "            Y_star1 = np.concatenate((X_new, Y_new), axis = 1)\n",
    "        \n",
    "        global_loss_train = loss_train_hl + loss_train_ll \n",
    "        # global loss function = combine the two loss functions\n",
    "        # optimizer for global loss function\n",
    "        params = list(high_layer.parameters()) + list(low_layer.parameters())\n",
    "        global_optimizer = optim.SGD(params, lr = args.lr, weight_decay = args.weight_decay)\n",
    "        global_optimizer.zero_grad()\n",
    "        \n",
    "        global_loss_train.backward(retain_graph = True)\n",
    "   \n",
    "        print(i)\n",
    "        global_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HSAIN KENZA\\Desktop\\Cours\\DD_TUM\\Praktikum\\Implementation_MLGCN\\Code\\models.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(L)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HSAIN KENZA\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-3002ad518b53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mglobal_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_star\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-149-12f8e2ea38cf>\u001b[0m in \u001b[0;36mglobal_train\u001b[1;34m(epochs, M, N, Y_star, X_star)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mglobal_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mglobal_loss_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "global_train(5, 2,3, Y_star, X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
