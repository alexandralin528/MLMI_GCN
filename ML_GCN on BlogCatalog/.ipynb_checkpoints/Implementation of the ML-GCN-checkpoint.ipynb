{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hsain kenza\\anaconda3\\lib\\site-packages (1.7.0+cpu)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\hsain kenza\\anaconda3\\lib\\site-packages (from torch) (0.6)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hsain kenza\\anaconda3\\lib\\site-packages (from torch) (3.7.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hsain kenza\\anaconda3\\lib\\site-packages (from torch) (1.18.5)\n",
      "Requirement already satisfied: future in c:\\users\\hsain kenza\\anaconda3\\lib\\site-packages (from torch) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to load the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the node-node-label graph**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is technically the original graph (constructed from *edges.csv*) with added edges. These added edges are defined by the labeling relationships with the common nodes and the label nodes. The idea would be to add the label nodes by giving them indices starting from $n+1$ to $n+40$. The edges will be deduced from the labeling relationships we have in the file *group-edges.csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    print(\"Loading {} dataset...\".format(data_name))\n",
    "    edges_file = data_name + \"/edges.csv\"\n",
    "    node_label_file = data_name + \"/group-edges.csv\"\n",
    "    nnlg_file = data_name + \"/edges_node_node_label.csv\"\n",
    "    llng_file = data_name + \"/edges_label_label_node.csv\"\n",
    "    # We'll first dive into the group_edges.csv files to extract the nodes and their corresponding labels for the training\n",
    "    label_raw, nodes = [], []\n",
    "    with open(node_label_file) as file_to_read: \n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break \n",
    "            node, label = lines.split(\",\")\n",
    "            label_raw.append(int(label))\n",
    "            nodes.append(int(node))\n",
    "\n",
    "    label_raw = np.array(label_raw)\n",
    "    nodes = np.array(nodes)\n",
    "    unique_nodes = np.unique(nodes)\n",
    "    labels = np.zeros((unique_nodes.shape[0], 39))\n",
    "    for l in range(1, 40, 1):\n",
    "        indices = np.argwhere(label_raw == l).reshape(-1)\n",
    "        n_l = nodes[indices]\n",
    "        for n in n_l:\n",
    "            labels[n-1][l-1] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "    # In this section, we will build our node-node-label graph \n",
    "    label_nodes = label_raw + unique_nodes.shape[0]\n",
    "    ## hereby are the nodes of the node-node-label graph \n",
    "    n_n_l_nodes = np.concatenate((unique_nodes, np.unique(label_nodes)))\n",
    "    ## Let's create a csv file with all the edges to create the graph \n",
    "    ## Let's begin with the common nodes\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(nnlg_file)\n",
    "    f = open(nnlg_file, \"r+\")\n",
    "    file_to_read = open(edges_file, \"r\")\n",
    "    f.writelines(file_to_read.readlines())\n",
    "    \n",
    "    ## Let's add the label edges\n",
    "    a = np.dstack((label_nodes,nodes)).reshape(label_nodes.shape[0],2)\n",
    "    e = [\",\".join(item)+\"\\n\" for item in a.astype(str)]\n",
    "    f.writelines(e)\n",
    "    \n",
    "    \n",
    "    # Now let's create the node-node-label graph \n",
    "    nnl_graph = nx.read_edgelist(f, delimiter = \",\", nodetype = int)\n",
    "    E = nx.adjacency_matrix(nnl_graph, nodelist = n_n_l_nodes)\n",
    "    E = sp.coo_matrix(E.todense())\n",
    "    \n",
    "    # Let's extract the feature matrix of the common nodes \n",
    "    A = nx.adjacency_matrix(nx.read_edgelist(file_to_read, delimiter = \",\", nodetype = int), nodelist = unique_nodes) \n",
    "    A = sp.coo_matrix(A.todense())\n",
    "    X = sp.csr_matrix(A)\n",
    "    \n",
    "    f.close()\n",
    "    file_to_read.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    # In this section, we will build our label-label-node graph\n",
    "    ## This graph is based on the co-occurence relationship between the labels\n",
    "    ### We'll use the matrix \"labels\" to extract this information for every pair of labels\n",
    "    ### Let's construct the edges between the labels\n",
    "    edges = []\n",
    "    list_edges = []\n",
    "    #C_1 = np.zeros((39,39))\n",
    "    for k in range(labels.shape[0]):\n",
    "        indices = np.argwhere(labels[k] == 1).reshape(-1)\n",
    "        if indices.shape[0] > 1:\n",
    "            for subset in itertools.combinations(indices, 2):\n",
    "                if (list(subset) not in list_edges) or ([subset[1], subset[0]] not in list_edges):\n",
    "                    list_edges.append([subset[0]+labels.shape[0], subset[1]+labels.shape[0]] )\n",
    "                    edges.append(str(subset[0]+1 + labels.shape[0]) + \",\" + str(subset[1] +1 + labels.shape[0]) + \"\\n\")\n",
    "                    #C_1[subset[0],subset[1]] = 1\n",
    "                    #C_1[subset[1],subset[0]] = 1\n",
    "                    \n",
    "    \n",
    "    df_1 = pd.DataFrame(list())\n",
    "    df_1.to_csv(llng_file)\n",
    "    f_1 = open(llng_file, \"r+\")\n",
    "    f_1.writelines(edges)\n",
    "    \n",
    "    # Let's extract the adjacency matrix of the cooccurences \n",
    "    C = nx.adjacency_matrix(nx.read_edgelist(f_1,delimiter=\",\", nodetype = int), nodelist = np.unique(label_raw + labels.shape[0]))\n",
    "    \n",
    "    labels_ind = label_raw + labels.shape[0]\n",
    "    a_1 = np.dstack((labels_ind,nodes)).reshape(labels_ind.shape[0],2)\n",
    "    e_1 = [\",\".join(item)+\"\\n\" for item in a_1.astype(str)]\n",
    "    \n",
    "    f_1.writelines(e_1)\n",
    "    \n",
    "    ## Now let's create a graph from what we just created \n",
    "    l_l_n_nodes = np.concatenate((np.unique(nodes),np.unique(labels_ind)))\n",
    "    lln_graph = nx.read_edgelist(f_1, delimiter = \",\", nodetype = int)\n",
    "    F = nx.adjacency_matrix(lln_graph, nodelist = l_l_n_nodes)\n",
    "    F = sp.coo_matrix(F.todense())\n",
    "    \n",
    "                \n",
    "    # In this section, we will extract useful matrices and vectors from our graph to feed the model\n",
    "    \n",
    "    \n",
    "    f_1.close()\n",
    "    \n",
    "    # add index_train and index_validation and index_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return E, F, X, C, labels, C_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BlogCatalog dataset...\n"
     ]
    }
   ],
   "source": [
    "E, F, X, C, labels, C_1 = load_data(\"BlogCatalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#from pygcn.utils import load_data, accuracy_sample, accuracy_sample_class\n",
    "from Code.models import high_layer_GCN, low_layer_GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Optimizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_train(epochs, N, M):\n",
    "    for i in range(epoch):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
