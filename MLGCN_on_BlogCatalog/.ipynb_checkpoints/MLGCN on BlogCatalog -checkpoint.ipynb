{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    edges_file = data_name + \"/edges.csv\"\n",
    "    node_label_file = data_name + \"/group-edges.csv\"\n",
    "    label_occ_file = data_name + \"/label_co-occurences.csv\"\n",
    "    nnlg_file = data_name + \"/edges_node_node_label.csv\"\n",
    "    llng_file = data_name + \"/edges_label_label_node.csv\"\n",
    "    label_raw, nodes = [], []\n",
    "    with open(node_label_file) as file_to_read: \n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break \n",
    "            node, label = lines.split(\",\")\n",
    "            label_raw.append(int(label))\n",
    "            nodes.append(int(node))\n",
    "    label_raw = np.array(label_raw)\n",
    "    nodes = np.array(nodes)\n",
    "    unique_nodes = np.unique(nodes)\n",
    "    labels = np.zeros((unique_nodes.shape[0], 39))\n",
    "    for l in range(1, 40, 1):\n",
    "        indices = np.argwhere(label_raw == l).reshape(-1)\n",
    "        n_l = nodes[indices]\n",
    "        for n in n_l:\n",
    "            labels[n-1][l-1] = 1\n",
    "            \n",
    "    label_nodes = label_raw + unique_nodes.shape[0] \n",
    "    n_n_l_nodes = np.concatenate((unique_nodes, np.unique(label_nodes)))\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(nnlg_file)\n",
    "    f = open(nnlg_file, \"r+\")\n",
    "    file_to_read = open(edges_file, \"r\")\n",
    "    f.writelines(file_to_read.readlines())\n",
    "    a = np.dstack((nodes, label_nodes)).reshape(label_nodes.shape[0],2)\n",
    "    e = [\"\\n\"] + [\",\".join(item)+\"\\n\" for item in a.astype(str)]\n",
    "    f.writelines(e)\n",
    "    f.close()\n",
    "    \n",
    "    nnlg_file = \"BlogCatalog/edges_node_node_label.csv\"\n",
    "    nnl_graph = nx.read_edgelist(nnlg_file, delimiter = \",\", nodetype = int)\n",
    "    E = nx.adjacency_matrix(nnl_graph, nodelist = n_n_l_nodes)\n",
    "    main_graph = open(edges_file, \"rb\")\n",
    "    G = nx.read_edgelist(main_graph, delimiter = \",\", nodetype = int)\n",
    "    A = nx.adjacency_matrix(G, nodelist = unique_nodes)\n",
    "    A = sp.coo_matrix(A.todense())\n",
    "    X = sp.csr_matrix(A)\n",
    "    A_tilde = normalize(A + sp.eye(A.shape[0]))\n",
    "    \n",
    "    # Label-label-node graph \n",
    "    edges = []\n",
    "    list_edges = []\n",
    "    for k in range(labels.shape[0]):\n",
    "        indices = np.argwhere(labels[k] == 1).reshape(-1)\n",
    "        if indices.shape[0]>1:\n",
    "            for subset in itertools.combinations(indices, 2): \n",
    "                if (list(subset) not in list_edges) or ([subset[1], subset[0]] not in list_edges):\n",
    "                    list_edges.append([subset[0]+labels.shape[0], subset[1]+labels.shape[0]])# check if the common nodes should be before or after the label nodes\n",
    "                    edges.append(str(subset[0]+1 + labels.shape[0]) + \",\" + str(subset[1] +1 + labels.shape[0]) + \"\\n\")\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(label_occ_file)\n",
    "    label_file = open(label_occ_file, \"r+\")\n",
    "    label_file.writelines(edges)\n",
    "    label_file.close()\n",
    "    unique_label_ID = np.arange(1,40) + labels.shape[0]\n",
    "    label_file = open(label_occ_file, \"rb\")\n",
    "    label_graph = nx.read_edgelist(label_file, delimiter = \",\", nodetype = int)\n",
    "    C = nx.adjacency_matrix(label_graph, nodelist = unique_label_ID)\n",
    "    label_file.close()\n",
    "    C_tilde = normalize(C + sp.eye(C.shape[0]))\n",
    "    Y = X[:39]\n",
    "    labels_ind = label_raw + labels.shape[0]\n",
    "    a_1 = np.dstack((labels_ind,nodes)).reshape(labels_ind.shape[0],2)\n",
    "    e_1 = [\",\".join(item)+\"\\n\" for item in a_1.astype(str)]\n",
    "    file = open(label_occ_file, \"r+\")\n",
    "    file.writelines(e_1)\n",
    "    file.close()\n",
    "    f_1 = open(label_occ_file, \"rb\")\n",
    "    l_l_n_nodes = np.concatenate((np.unique(nodes),np.unique(labels_ind)))\n",
    "    lln_graph = nx.read_edgelist(f_1, delimiter = \",\", nodetype = int)\n",
    "    F = nx.adjacency_matrix(lln_graph, nodelist = l_l_n_nodes)\n",
    "    F = sp.coo_matrix(F.todense())\n",
    "    f_1.close()\n",
    "    E = normalize(E + sp.eye(E.shape[0]))\n",
    "    E_tilde = E[:len(unique_nodes)]\n",
    "    F = normalize(F + sp.eye(F.shape[0]))\n",
    "    F_tilde = F[len(unique_nodes):]\n",
    "    indices = np.arange(A.shape[0]).astype('int32')\n",
    "    idx_train = indices[:A.shape[0] // 3]\n",
    "    idx_val = indices[A.shape[0] // 3: (2 * A.shape[0]) // 3]\n",
    "    idx_test = indices[(2 * A.shape[0]) // 3:]\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    C_tilde = torch.FloatTensor(np.array(C_tilde.todense()))\n",
    "    E_tilde = torch.FloatTensor(np.array(E_tilde.todense()))\n",
    "    F_tilde = torch.FloatTensor(np.array(F_tilde.todense()))\n",
    "    A_tilde = torch.FloatTensor(np.array(A_tilde.todense()))\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    return X, Y, F_tilde, E_tilde, C_tilde,A_tilde, idx_train, idx_val,idx_test, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, F_tilde, E_tilde, C_tilde, A_tilde, idx_train, idx_val,idx_test, labels = load_data(\"BlogCatalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_star = sp.vstack((X,Y))\n",
    "Y_star = sp.vstack((Y,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get down to the training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code.models import High_Layer, Low_Layer\n",
    "import time\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.02,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=400,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(output):\n",
    "    output[output > 0.5] = 1\n",
    "    output[output <= 0.5] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_star = torch.FloatTensor(sp.vstack((Y,X)).todense())\n",
    "X_star = torch.FloatTensor(sp.vstack((X,Y)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_high_layer(Y_star):\n",
    "    high_layer = High_Layer(nfeat = A_tilde.shape[0],\n",
    "                            nhid = args.hidden,\n",
    "                            nclass = C_tilde.shape[0],\n",
    "                            dropout = args.dropout)\n",
    "    optimizer_highLayer = optim.SGD(high_layer.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    high_layer.train()\n",
    "    optimizer_highLayer.zero_grad()\n",
    "    Y_new = high_layer(Y_star, F_tilde, C_tilde)\n",
    "    print(Y_new.shape)\n",
    "    # Calculate the train loss (Cross-Entropy)\n",
    "    truth = torch.LongTensor(np.arange(39))\n",
    "    loss = F.cross_entropy(F.sigmoid(Y_new), truth) \n",
    "    print(type(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    optimizer_highLayer.step()\n",
    "    \n",
    "    \n",
    "    return Y_new, loss, high_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 39])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HSAIN KENZA\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0370,  0.0292,  0.0389,  ...,  0.0956,  0.0050, -0.0689],\n",
       "         [-0.0345,  0.0289,  0.0393,  ...,  0.0958,  0.0021, -0.0696],\n",
       "         [-0.0348,  0.0283,  0.0412,  ...,  0.0960,  0.0033, -0.0704],\n",
       "         ...,\n",
       "         [-0.0285,  0.0290,  0.0327,  ...,  0.0924,  0.0028, -0.0689],\n",
       "         [-0.0468,  0.0265,  0.0125,  ...,  0.0876,  0.0205, -0.0644],\n",
       "         [ 0.0171,  0.0618,  0.1562,  ...,  0.3220,  0.0439, -0.1104]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor(3.6641, grad_fn=<NllLossBackward>),\n",
       " High_Layer(\n",
       "   (gc1): GraphConvolution (10312 -> 400)\n",
       "   (gc2): GraphConvolution (400 -> 39)\n",
       " ))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_high_layer(Y_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sample_class(output, labels):\n",
    "    \"\"\" \n",
    "    output is of shape (N,C)\n",
    "    Labels is of shape (N,C)\n",
    "    Result : acc gives the accuracy computed according to the sample-class view\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    C = labels.shape[1]\n",
    "    corr = np.sum(np.equal(output, labels))\n",
    "    # corr is the number of equal elements between labels and output and thus the number of correctly classified \n",
    "    # labels for each sample \n",
    "    acc = corr/(N*C)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_low_layer(X_star):\n",
    "    \n",
    "    lowï¼¿layer = Low_Layer(nfeat = A_tilde.shape[0],\n",
    "                           nhid = args.hidden,\n",
    "                           nclass = C_tilde.shape[0],\n",
    "                           dropout = args.dropout)\n",
    "    optimizer_lowLayer = optim.SGD(low_layer.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    low_layer.train()\n",
    "    optimizer_lowLayer.zero_grad()\n",
    "    X_new = low_layer(X_star, E_tilde, A_tilde)\n",
    "    X_new = X_new.detach()\n",
    "    # Calculate the train loss (Binary Cross Entropy)\n",
    "    loss_train = torch.from_numpy(np.array((1/39)*np.sum([F.binary_cross_entropy_with_logits(X_new[idx_train][:,i], labels[idx_train][:,i]) for i in range(C_tilde.shape[0])])))\n",
    "    loss_train.requires_grad = True\n",
    "    acc_train = accuracy_sample_class(threshold(X_new.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer_lowLayer.step()\n",
    "    \n",
    "    softmax_output = F.log_softmax(X_new, dim = 1)\n",
    "    loss_val = (1/39)*np.sum([F.binary_cross_entropy_with_logits(softmax_output[idx_val][:,i], labels[idx_val][:,i]) for i in range(C_tilde.shape[0])])\n",
    "    \n",
    "    return X_new, loss_train, loss_val, acc_train, low_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1759, -0.1681,  0.0083,  ..., -0.2670, -0.0513, -0.0374],\n",
       "         [-0.1783, -0.1382,  0.1404,  ..., -0.3627, -0.0356,  0.0301],\n",
       "         [-0.1366, -0.1205,  0.0521,  ..., -0.2755, -0.0981,  0.0169],\n",
       "         ...,\n",
       "         [-0.1959, -0.1809,  0.0014,  ..., -0.3545,  0.0298, -0.0084],\n",
       "         [-0.2511, -0.2470, -0.0791,  ..., -0.4475,  0.1162, -0.1687],\n",
       "         [-0.1379, -0.4489,  0.1119,  ..., -0.3143,  0.0672, -0.3547]]),\n",
       " tensor(0.6807, dtype=torch.float64, requires_grad=True),\n",
       " 0.15802659743871444,\n",
       " 0.9456368478771737,\n",
       " Low_Layer(\n",
       "   (gc1): GraphConvolution (10312 -> 400)\n",
       "   (gc2): GraphConvolution (400 -> 39)\n",
       " ))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_low_layer(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_layer = Low_Layer(nfeat = A_tilde.shape[0],\n",
    "                           nhid = args.hidden,\n",
    "                           nclass = C_tilde.shape[0],\n",
    "                           dropout = args.dropout)\n",
    "optimize_lowlayer = optim.SGD(low_layer.parameters(),lr = args.lr, weight_decay = args.weight_decay)\n",
    "        \n",
    "high_layer = High_Layer(nfeat = A_tilde.shape[0],\n",
    "                            nhid = args.hidden,\n",
    "                            nclass = C_tilde.shape[0],\n",
    "                            dropout = args.dropout)\n",
    "optimize_highlayer = optim.SGD(high_layer.parameters(),lr = args.lr, weight_decay = args.weight_decay)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training procedure that occurs in one epoch\n",
    "def train_model(epoch):\n",
    "    t = time.time()\n",
    "    low_layer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global train function for having a loop \n",
    "# epochs M N for\n",
    "def global_train(epochs, M, N, X, Y, X_star, Y_star):\n",
    "    \n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        if i == 0:\n",
    "            low_layer = Low_Layer(nfeat = A_tilde.shape[0],\n",
    "                           nhid = args.hidden,\n",
    "                           nclass = C_tilde.shape[0],\n",
    "                           dropout = args.dropout)\n",
    "            high_layer = High_Layer(nfeat = A_tilde.shape[0],\n",
    "                            nhid = args.hidden,\n",
    "                            nclass = C_tilde.shape[0],\n",
    "                            dropout = args.dropout)\n",
    "        \n",
    "        #Y_new = high_layer(Y_star, F_tilde, C_tilde)\n",
    "        #X_new = low_layer(X_star, E_tilde, A_tilde)\n",
    "        \n",
    "        #truth = torch.LongTensor(np.arange(39))\n",
    "        #loss = F.cross_entropy(F.sigmoid(Y_new), truth) \n",
    "        #loss_train = torch.from_numpy(np.array((1/39)*np.sum([F.binary_cross_entropy_with_logits(X_new[idx_train][:,i], labels[idx_train][:,i]) for i in range(C_tilde.shape[0])])))\n",
    "        \n",
    "        \n",
    "        if i%M:\n",
    "            Y_new, loss_hl, high_layer = train_high_layer(Y_star)\n",
    "            X_star = np.concatenate((X, Y_new),axis = 1)\n",
    "            \n",
    "        elif i%N:\n",
    "            X_new, loss_train_ll, loss_val_ll, acc_train_ll, low_layer = train_low_layer(X_star)\n",
    "            Y_star = np.concatenate((Y, X_new), axis = 1)\n",
    "        else:\n",
    "            Y_new = high_layer(Y_star, F_tilde, C_tilde)\n",
    "            X_new = low_layer(X_star, E_tilde, A_tilde)\n",
    "        \n",
    "            truth = torch.LongTensor(np.arange(39))\n",
    "            loss_hl = F.cross_entropy(F.sigmoid(Y_new), truth) \n",
    "            loss_train_ll = torch.from_numpy(np.array((1/39)*np.sum([F.binary_cross_entropy_with_logits(X_new.detach().numpy()[idx_train][:,i], labels[idx_train][:,i]) for i in range(C_tilde.shape[0])])))\n",
    "            \n",
    "        X_star = X_star.detach()\n",
    "        Y_star = Y_star.detach()\n",
    "        \n",
    "        global_loss_train = loss_hl + loss_train_ll \n",
    "        # global loss function = combine the two loss functions\n",
    "        # optimizer for global loss function\n",
    "        # params = list(high_layer.parameters()) + list(low_layer.parameters())\n",
    "        global_optimizer = optim.SGD(low_layer.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "        global_optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        global_loss_train.backward()\n",
    "   \n",
    "        print(i)\n",
    "        global_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_train(5, 2,3, X,Y, X_star, Y_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
