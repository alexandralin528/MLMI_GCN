{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(output):\n",
    "    output[output > 0.5] = 1\n",
    "    output[output <= 0.5] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sample_class(output, labels):\n",
    "    \"\"\" \n",
    "    output is of shape (N,C)\n",
    "    Labels is of shape (N,C)\n",
    "    Result : acc gives the accuracy computed according to the sample-class view\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    C = labels.shape[1]\n",
    "    corr = np.sum(np.equal(output, labels))\n",
    "    # corr is the number of equal elements between labels and output and thus the number of correctly classified \n",
    "    # labels for each sample \n",
    "    acc = corr/(N*C)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_F1(output, labels):\n",
    "    TP = np.array([])\n",
    "    FN = np.array([])\n",
    "    FP = np.array([])\n",
    "    N = labels.shape[0]\n",
    "    L = labels.shape[1]\n",
    "    for l in range(L):\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        fp = 0\n",
    "        ol = output[:,l]\n",
    "        ll = labels[:,l]\n",
    "        for i in range(N):\n",
    "            if ll[i] == 1 and ol[i] == 1:\n",
    "                tp +=1\n",
    "            if ol[i] == 0 and ll[i] == 1:\n",
    "                fn +=1\n",
    "            if ol[i] == 1 and ll[i] == 0:\n",
    "                fp +=1\n",
    "        TP = np.append(TP, tp)\n",
    "        FN = np.append(FN, fn)\n",
    "        FP = np.append(FP, fp)\n",
    "    return np.sum(2*TP)/np.sum(2*TP + FN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,0,1],\n",
    "              [1,0,1]])\n",
    "b = np.array([[1,0,1],\n",
    "              [1,0,1]])\n",
    "micro_F1(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_F1(output, labels):\n",
    "    TP = np.array([])\n",
    "    FN = np.array([])\n",
    "    FP = np.array([])\n",
    "    N = labels.shape[0]\n",
    "    L = labels.shape[1]\n",
    "    for l in range(L):\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        fp = 0\n",
    "        ol = output[:,l]\n",
    "        ll = labels[:,l]\n",
    "        for i in range(N):\n",
    "            if ll[i] == 1 and ol[i] == 1:\n",
    "                tp +=1\n",
    "            if ol[i] == 0 and ll[i] == 1:\n",
    "                fn +=1\n",
    "            if ol[i] == 1 and ll[i] == 0:\n",
    "                fp +=1\n",
    "        TP = np.append(TP, tp)\n",
    "        FN = np.append(FN, fn)\n",
    "        FP = np.append(FP, fp)\n",
    "    return np.sum(2*TP/(2*TP + FN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of labels\n",
    "m = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    edges_file = data_name + \"/edges.csv\"\n",
    "    node_label_file = data_name + \"/group-edges.csv\"\n",
    "    label_occ_file = data_name + \"/label_co-occurences.csv\"\n",
    "    nnlg_file = data_name + \"/edges_node_node_label.csv\"\n",
    "    llng_file = data_name + \"/edges_label_label_node.csv\"\n",
    "    label_raw, nodes = [], []\n",
    "    with open(node_label_file) as file_to_read: \n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break \n",
    "            node, label = lines.split(\",\")\n",
    "            label_raw.append(int(label))\n",
    "            nodes.append(int(node))\n",
    "    label_raw = np.array(label_raw)\n",
    "    nodes = np.array(nodes)\n",
    "    unique_nodes = np.unique(nodes)\n",
    "    labels = np.zeros((unique_nodes.shape[0], m))\n",
    "    for l in range(1, m+1, 1):\n",
    "        indices = np.argwhere(label_raw == l).reshape(-1)\n",
    "        n_l = nodes[indices]\n",
    "        for n in n_l:\n",
    "            labels[n-1][l-1] = 1\n",
    "            \n",
    "    label_nodes = label_raw + unique_nodes.shape[0] \n",
    "    n_n_l_nodes = np.concatenate((unique_nodes, np.unique(label_nodes)))\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(nnlg_file)\n",
    "    f = open(nnlg_file, \"r+\")\n",
    "    file_to_read = open(edges_file, \"r\")\n",
    "    f.writelines(file_to_read.readlines())\n",
    "    a = np.dstack((nodes, label_nodes)).reshape(label_nodes.shape[0],2)\n",
    "    e = [\"\\n\"] + [\",\".join(item)+\"\\n\" for item in a.astype(str)]\n",
    "    f.writelines(e)\n",
    "    f.close()\n",
    "    \n",
    "    nnlg_file = \"BlogCatalog/edges_node_node_label.csv\"\n",
    "    nnl_graph = nx.read_edgelist(nnlg_file, delimiter = \",\", nodetype = int)\n",
    "    E = nx.adjacency_matrix(nnl_graph, nodelist = n_n_l_nodes)\n",
    "    main_graph = open(edges_file, \"rb\")\n",
    "    G = nx.read_edgelist(main_graph, delimiter = \",\", nodetype = int)\n",
    "    A = nx.adjacency_matrix(G, nodelist = unique_nodes)\n",
    "    A = sp.coo_matrix(A.todense())\n",
    "    X = sp.csr_matrix(A)\n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    A_tilde = normalize(A + sp.eye(A.shape[0]))\n",
    "    \n",
    "    # Label-label-node graph \n",
    "    edges = []\n",
    "    list_edges = []\n",
    "    for k in range(labels.shape[0]):\n",
    "        indices = np.argwhere(labels[k] == 1).reshape(-1)\n",
    "        if indices.shape[0]>1:\n",
    "            for subset in itertools.combinations(indices, 2): \n",
    "                if (list(subset) not in list_edges) or ([subset[1], subset[0]] not in list_edges):\n",
    "                    list_edges.append([subset[0]+labels.shape[0], subset[1]+labels.shape[0]])# check if the common nodes should be before or after the label nodes\n",
    "                    edges.append(str(subset[0]+1 + labels.shape[0]) + \",\" + str(subset[1] +1 + labels.shape[0]) + \"\\n\")\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(label_occ_file)\n",
    "    label_file = open(label_occ_file, \"r+\")\n",
    "    label_file.writelines(edges)\n",
    "    label_file.close()\n",
    "    unique_label_ID = np.arange(1,40) + labels.shape[0]\n",
    "    label_file = open(label_occ_file, \"rb\")\n",
    "    label_graph = nx.read_edgelist(label_file, delimiter = \",\", nodetype = int)\n",
    "    C = nx.adjacency_matrix(label_graph, nodelist = unique_label_ID)\n",
    "    label_file.close()\n",
    "    # Normalize the co-occurence matrix with the added self loops\n",
    "    C_tilde = normalize(C + sp.eye(C.shape[0]))\n",
    "    Y = X[:m]\n",
    "    labels_ind = label_raw + labels.shape[0]\n",
    "    a_1 = np.dstack((labels_ind,nodes)).reshape(labels_ind.shape[0],2)\n",
    "    e_1 = [\",\".join(item)+\"\\n\" for item in a_1.astype(str)]\n",
    "    file = open(label_occ_file, \"r+\")\n",
    "    file.writelines(e_1)\n",
    "    file.close()\n",
    "    f_1 = open(label_occ_file, \"rb\")\n",
    "    l_l_n_nodes = np.concatenate((np.unique(nodes),np.unique(labels_ind)))\n",
    "    lln_graph = nx.read_edgelist(f_1, delimiter = \",\", nodetype = int)\n",
    "    F = nx.adjacency_matrix(lln_graph, nodelist = l_l_n_nodes)\n",
    "    F = sp.coo_matrix(F.todense())\n",
    "    f_1.close()\n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    E = normalize(E + sp.eye(E.shape[0]))\n",
    "    E_tilde = E[:len(unique_nodes)]\n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    F = normalize(F + sp.eye(F.shape[0]))\n",
    "    F_tilde = F[len(unique_nodes):]\n",
    "    \n",
    "    indices = np.arange(A.shape[0]).astype('int32') # should be shuffled\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    idx_train = indices[:A.shape[0] // 3]\n",
    "    idx_val = indices[A.shape[0] // 3: (2 * A.shape[0]) // 3]\n",
    "    idx_test = indices[(2 * A.shape[0]) // 3:]\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    C_tilde = torch.FloatTensor(np.array(C_tilde.todense()))\n",
    "    E_tilde = torch.FloatTensor(np.array(E_tilde.todense()))\n",
    "    F_tilde = torch.FloatTensor(np.array(F_tilde.todense()))\n",
    "    A_tilde = torch.FloatTensor(np.array(A_tilde.todense()))\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    Y = torch.FloatTensor(np.array(Y.todense()))\n",
    "    X = torch.FloatTensor(np.array(X.todense()))\n",
    "    \n",
    "    return X, Y, F_tilde, E_tilde, C_tilde,A_tilde, idx_train, idx_val,idx_test, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, F_tilde, E_tilde, C_tilde, A_tilde, idx_train, idx_val,idx_test, labels = load_data(\"BlogCatalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_star = torch.FloatTensor(sp.vstack((Y,X)).todense())\n",
    "X_star = torch.FloatTensor(sp.vstack((X,Y)).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get down to the training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code.models import High_Layer, Low_Layer\n",
    "import time\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.02,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=400,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables \n",
    "n = A_tilde.shape[0]\n",
    "m = C_tilde.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models (high layer and low layer) and the optimizers for each on of them\n",
    "low_layer = Low_Layer(nfeat = A_tilde.shape[0],\n",
    "                      nhid = args.hidden,\n",
    "                      nclass = C_tilde.shape[0],\n",
    "                      dropout = args.dropout)\n",
    "optimizer_lowlayer = optim.SGD(low_layer.parameters(),lr = args.lr, weight_decay = args.weight_decay)\n",
    "        \n",
    "high_layer = High_Layer(nfeat = A_tilde.shape[0],\n",
    "                        nhid = args.hidden,\n",
    "                        nclass = C_tilde.shape[0],\n",
    "                        dropout = args.dropout)\n",
    "optimizer_highlayer = optim.SGD(high_layer.parameters(),lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = torch.LongTensor(np.arange(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 10312])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 39])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_layer.gc2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10312, 39])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_layer.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, M, N):# Put M and N in args \n",
    "    \n",
    "    X_star1 = X_star\n",
    "    Y_star1 = Y_star\n",
    "    t = time.time()\n",
    "    # train the two layers\n",
    "    low_layer.train()\n",
    "    optimizer_lowlayer.zero_grad()\n",
    "    high_layer.train()\n",
    "    optimizer_highlayer.zero_grad()\n",
    "    # The output of the two layers\n",
    "    Y_new = high_layer(Y_star1, F_tilde, C_tilde)\n",
    "    X_new = low_layer(X_star1, E_tilde, A_tilde)\n",
    "    print(X_new.shape)\n",
    "    # The train losses\n",
    "    loss_train_hl = F.cross_entropy(torch.sigmoid(Y_new), truth)\n",
    "    softmax_output = F.log_softmax(X_new, dim = 1)\n",
    "    loss_train_ll = (1/m)*np.sum([F.binary_cross_entropy_with_logits(softmax_output[idx_train][:,i], labels[idx_train][:,i]) for i in range(m)])\n",
    "    \n",
    "    print(type(Y))\n",
    "    print(type(X_new))\n",
    "    acc_train = accuracy_sample_class(threshold(softmax_output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    if epoch%M == 0:\n",
    "        W_l = torch.transpose(high_layer.fc1.weight, 0, 1)\n",
    "        Y_new1 = torch.mm(Y_new, W_l)\n",
    "        Y_star1 = torch.cat((X, Y_new1), dim = 0)\n",
    "    if epoch%N == 0:\n",
    "        W_v = torch.transpose(low_layer.fc1.weight, 0, 1)\n",
    "        Y_new1 = torch.mm(X_new, W_v)\n",
    "        X_star1 = torch.cat((X,Y_new1), dim = 0)\n",
    "        \n",
    "    \n",
    "    loss_train_ll.backward()\n",
    "    loss_train_hl.backward()\n",
    "    optimizer_lowlayer.step()\n",
    "    optimizer_highlayer.step()\n",
    "    \n",
    "    \n",
    "    loss_val = (1/39)*np.sum([F.binary_cross_entropy_with_logits(softmax_output[idx_val][:,i], labels[idx_val][:,i]) for i in range(C_tilde.shape[0])])\n",
    "    acc_val = accuracy_sample_class(threshold(softmax_output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train_ll.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(epoch, M, N):# Put M and N in args \n",
    "    \n",
    "    X_star1 = X_star\n",
    "    Y_star1 = Y_star\n",
    "    t = time.time()\n",
    "    # train the two layers\n",
    "    low_layer.train()\n",
    "    optimizer_lowlayer.zero_grad()\n",
    "    high_layer.train()\n",
    "    optimizer_highlayer.zero_grad()\n",
    "    # The output of the two layers\n",
    "    Y_new = high_layer(Y_star1, F_tilde, C_tilde)\n",
    "    X_new = low_layer(X_star1, E_tilde, A_tilde)\n",
    "    # The train losses\n",
    "    loss_train_hl = F.cross_entropy(F.log_softmax(Y_new,dim = 1), truth)\n",
    "    sigmoid_output = torch.sigmoid(X_new)\n",
    "    loss_train_ll = (1/m)*np.sum([F.binary_cross_entropy_with_logits(sigmoid_output[idx_train][:,i], labels[idx_train][:,i]) for i in range(m)])\n",
    "    \n",
    "    \n",
    "    acc_train = macro_F1(threshold(sigmoid_output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    if epoch%M == 0:\n",
    "        W_l = torch.transpose(high_layer.fc1.weight, 0, 1)\n",
    "        Y_new1 = torch.mm(Y_new, W_l)\n",
    "        Y_star1 = torch.cat((X, Y_new1), dim = 0)\n",
    "    if epoch%N == 0:\n",
    "        W_v = torch.transpose(low_layer.fc1.weight, 0, 1)\n",
    "        Y_new1 = torch.mm(X_new, W_v)\n",
    "        X_star1 = torch.cat((X,Y_new1), dim = 0)\n",
    "        \n",
    "    \n",
    "    loss_train_ll.backward()\n",
    "    loss_train_hl.backward()\n",
    "    optimizer_lowlayer.step()\n",
    "    optimizer_highlayer.step()\n",
    "    \n",
    "    \n",
    "    loss_val = (1/m)*np.sum([F.binary_cross_entropy_with_logits(sigmoid_output[idx_val][:,i], labels[idx_val][:,i]) for i in range(C_tilde.shape[0])])\n",
    "    acc_val = macro_F1(threshold(sigmoid_output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train_ll.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_layer = Low_Layer(nfeat = A_tilde.shape[0],\n",
    "                      nhid = args.hidden,\n",
    "                      nclass = C_tilde.shape[0],\n",
    "                      dropout = args.dropout)\n",
    "\n",
    "high_layer = High_Layer(nfeat = A_tilde.shape[0],\n",
    "                        nhid = args.hidden,\n",
    "                        nclass = C_tilde.shape[0],\n",
    "                        dropout = args.dropout)\n",
    "params = list(high_layer.parameters()) + list(low_layer.parameters())\n",
    "optimizer = optim.SGD(params,lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(epoch, M, N):# Put M and N in args \n",
    "    \n",
    "    X_star1 = X_star\n",
    "    Y_star1 = Y_star\n",
    "    t = time.time()\n",
    "    # train the two layers\n",
    "    low_layer.train()\n",
    "    high_layer.train()\n",
    "    optimizer.zero_grad()\n",
    "    # The output of the two layers\n",
    "    Y_new = high_layer(Y_star1, F_tilde, C_tilde)\n",
    "    X_new = low_layer(X_star1, E_tilde, A_tilde)\n",
    "    # The train losses\n",
    "    loss_train_hl = F.cross_entropy(F.log_softmax(Y_new,dim = 1), truth)\n",
    "    sigmoid_output = torch.sigmoid(X_new)\n",
    "    loss_train_ll = (1/m)*np.sum([F.binary_cross_entropy_with_logits(sigmoid_output[idx_train][:,i], labels[idx_train][:,i]) for i in range(m)])\n",
    "    \n",
    "    #print(threshold(sigmoid_output))\n",
    "    acc_train = micro_F1(threshold(sigmoid_output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    if epoch%M == 0:\n",
    "        W_l = torch.transpose(high_layer.fc1.weight, 0, 1)\n",
    "        Y_new1 = torch.mm(Y_new, W_l)\n",
    "        Y_star1 = torch.cat((X, Y_new1), dim = 0)\n",
    "    if epoch%N == 0:\n",
    "        W_v = torch.transpose(low_layer.fc1.weight, 0, 1)\n",
    "        Y_new1 = torch.mm(X_new, W_v)\n",
    "        X_star1 = torch.cat((X,Y_new1), dim = 0)\n",
    "        \n",
    "    \n",
    "    loss_train = loss_train_hl + loss_train_ll\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    loss_val = (1/m)*np.sum([F.binary_cross_entropy_with_logits(sigmoid_output[idx_val][:,i], labels[idx_val][:,i]) for i in range(C_tilde.shape[0])])\n",
    "    acc_val = micro_F1(threshold(sigmoid_output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002 loss_train: 4.6223 acc_train: 0.0592 loss_val: 0.9487 acc_val: 0.0594 time: 14.6953s\n"
     ]
    }
   ],
   "source": [
    "train2(1, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 4.6232 acc_train: 0.0605 loss_val: 0.9478 acc_val: 0.0616 time: 20.1691s\n",
      "Epoch: 0002 loss_train: 4.6228 acc_train: 0.0583 loss_val: 0.9473 acc_val: 0.0587 time: 9.2118s\n",
      "Epoch: 0003 loss_train: 4.6213 acc_train: 0.0579 loss_val: 0.9464 acc_val: 0.0599 time: 9.8017s\n",
      "Epoch: 0004 loss_train: 4.6202 acc_train: 0.0593 loss_val: 0.9458 acc_val: 0.0606 time: 8.2963s\n",
      "Epoch: 0005 loss_train: 4.6175 acc_train: 0.0593 loss_val: 0.9445 acc_val: 0.0607 time: 9.9459s\n",
      "Epoch: 0006 loss_train: 4.6183 acc_train: 0.0600 loss_val: 0.9442 acc_val: 0.0608 time: 10.9338s\n",
      "Epoch: 0007 loss_train: 4.6155 acc_train: 0.0602 loss_val: 0.9432 acc_val: 0.0599 time: 8.7221s\n",
      "Epoch: 0008 loss_train: 4.6152 acc_train: 0.0589 loss_val: 0.9425 acc_val: 0.0603 time: 7.8554s\n",
      "Epoch: 0009 loss_train: 4.6133 acc_train: 0.0591 loss_val: 0.9413 acc_val: 0.0603 time: 8.4021s\n",
      "Epoch: 0010 loss_train: 4.6096 acc_train: 0.0590 loss_val: 0.9404 acc_val: 0.0610 time: 10.1922s\n",
      "Epoch: 0011 loss_train: 4.6140 acc_train: 0.0598 loss_val: 0.9397 acc_val: 0.0608 time: 8.7659s\n",
      "Epoch: 0012 loss_train: 4.6101 acc_train: 0.0595 loss_val: 0.9388 acc_val: 0.0581 time: 9.6569s\n",
      "Epoch: 0013 loss_train: 4.6091 acc_train: 0.0591 loss_val: 0.9381 acc_val: 0.0595 time: 8.7741s\n",
      "Epoch: 0014 loss_train: 4.6103 acc_train: 0.0591 loss_val: 0.9373 acc_val: 0.0601 time: 7.9600s\n",
      "Epoch: 0015 loss_train: 4.6092 acc_train: 0.0593 loss_val: 0.9366 acc_val: 0.0587 time: 8.6324s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-307ce0c0f927>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-6d522c2b3a25>\u001b[0m in \u001b[0;36mtrain2\u001b[1;34m(epoch, M, N)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# The output of the two layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mY_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhigh_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_star1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF_tilde\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC_tilde\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlow_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_star1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE_tilde\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_tilde\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# The train losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mloss_train_hl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         for hook in itertools.chain(\n\u001b[0m\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m                 self._forward_hooks.values()):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train2(epoch, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
