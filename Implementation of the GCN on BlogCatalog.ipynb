{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the BlogCatalog Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our part, we chose to work with the BlogCatalog dataset. It is a graph dateset that represents a network of social relationships, where the nodes represent blogger authors and the labels reprensent the bloggers' interest such as *Education*, *Food* and *Health*. The problem we are trying to solve is a multi-label classification of nodes which means that a node (blogger) might have one corresponding label (interest). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into our ML-GCN method for multi label classification on graphs, we'll first take a look at our dataset, to get a better understanding of it and make the implementation easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the paper, the graph has over 10312 nodes with over 333983 edges connecting them. For each node, there 39 possible labels with iver 615 co-occurence relationships between them. \n",
    "The blog dataset is given in the form of two main csv files. The first one is called group-edges.csv. It gives the nodes and their corresponding labels. It should be noted that each node might have more than one label. The first step before moving along with classification would be to attribute each nodes its corresponding labels in the form of 39 lenghted vector, with the value 1 in the index corresponding to a related label and 0 otherwise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second file is called edges.csv and as its name indicates it represents edge relationships between different nodes to form the initial graph. This graph will be used to build the node graph along with extracting the adjacency matrix and the features vector of each one of the nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One first important step in any deep learning method is to load the dataset to make it usable for our deep/machine learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing that, it's important to know what we need for our algorithm (a simple GCN at first). As an input for our model, we will need the adjacency matrix of the graph (normalized with added self loops) $\\hat{A}$ and the features matrix $X$. To compute the train/validation loss, we will need the ground truth labels for the training/validation dataset. Last but not least, to feed our dataset to our GCN model, we need our set of nodes split somehow to three subsets for training, validation and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name): \n",
    "    print(\"Loading {} dataset...\".format(data_name))\n",
    "    edges_file = data_name + \"/edges.csv\"\n",
    "    node_label_file = data_name + \"/group-edges.csv\"\n",
    "    \n",
    "    # We'll first dive into the group_edges.csv file in order to extract a list of the nodes along with their \n",
    "    # corresponding labels\n",
    "    label_raw, nodes = [], []\n",
    "    with open(node_label_file) as file_to_read: \n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break \n",
    "            node, label = lines.split(\",\")\n",
    "            label_raw.append(int(label))\n",
    "            nodes.append(int(node))\n",
    "    unique_nodes = np.unique(nodes)\n",
    "    # Now we have a list of nodes and a list of their labels\n",
    "    # Since a node can have multiple labels, we should give each node a corresponding 39 lengthed vector that \n",
    "    # encodes 1 when the node has the label corresponding to the index and 0 otherwise.  \n",
    "    label_raw = np.array(label_raw)\n",
    "    nodes = np.array(nodes)\n",
    "    labels = np.zeros((unique_nodes.shape[0], 39))\n",
    "    for l in range(1, 40, 1):\n",
    "        indices = np.argwhere(label_raw == l).reshape(-1)\n",
    "        n_l = nodes[indices]\n",
    "        for n in n_l:\n",
    "            labels[n-1][l-1] = 1\n",
    "    \n",
    "    # Now we can build our BlogCatalog graph using the file edges.csv \n",
    "    \n",
    "    file_to_read = open(edges_file, 'rb')\n",
    "    print(file_to_read)\n",
    "    G = nx.read_edgelist(file_to_read, delimiter = \",\", nodetype = int)\n",
    "    \n",
    "    # Let's now extract our adjacency matrix from the graph \n",
    "    A = nx.adjacency_matrix(G, nodelist = unique_nodes) # Already a symmetric matrix \n",
    "    A = sp.coo_matrix(A.todense())\n",
    "    \n",
    "    # Let's extract the feature matrix as well\n",
    "    X = sp.csr_matrix(A)\n",
    "    \n",
    "    # As we saw in the paper, we need the normalized version of the adjacency matrix with the added self loops\n",
    "    A = normalize(A + sp.eye(A.shape[0]))\n",
    "    # X = normalize(X) --> Why do we need to do that ? \n",
    "    \n",
    "    # Let's define the train, validation and test sets \n",
    "    indices = np.arange(A.shape[0]).astype('int32')\n",
    "    # np.random.shuffle(indices)\n",
    "    idx_train = indices[:A.shape[0] // 3]\n",
    "    idx_val = indices[A.shape[0] // 3: (2 * A.shape[0]) // 3]\n",
    "    idx_test = indices[(2 * A.shape[0]) // 3:]\n",
    "    \n",
    "    # Convert to tensors \n",
    "    X = torch.FloatTensor(np.array(X.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "    A = sparse_mx_to_torch_sparse_tensor(A)\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    return A, X, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BlogCatalog dataset...\n",
      "<_io.BufferedReader name='BlogCatalog/edges.csv'>\n"
     ]
    }
   ],
   "source": [
    "A, X, labels, idx_train, idx_val, idx_test = load_data(\"BlogCatalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function that gives the accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple accuracy is equal to the number of correct predictions over the number of samples, in the multi-label classification, the accuracy can be defined in 2 ways: \n",
    "\n",
    "- Sample view : A sample is correctly classified when all of its labels are correctly classified and accuracy means the number of correctly classified samples over the total number of samples.\n",
    "- Sample-class view : Assume that the output of the classifier is a matrix with dimension N by C where N is the number of samples and C is the number of classes. Accuracy means how many of the N by C elements in this output are correctly classified elements divided by the number of elements of the matrix. \n",
    "\n",
    "We can implement both views in the following way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sample(output, labels):\n",
    "    \"\"\" \n",
    "    output and labels are tensors\n",
    "    output is of shape (N,C)\n",
    "    Labels is of shape (N,C)\n",
    "    Result : acc gives the accuracy computed according to the sample view\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    corr = np.sum(np.all(np.equal(output, labels), axis=1))\n",
    "    # corr is the number of equal rows and thus the number of correctly classified samples\n",
    "    acc = corr / N\n",
    "    return acc       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sample_class(output, labels):\n",
    "    \"\"\" \n",
    "    output is of shape (N,C)\n",
    "    Labels is of shape (N,C)\n",
    "    Result : acc gives the accuracy computed according to the sample-class view\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    C = labels.shape[1]\n",
    "    corr = np.sum(np.equal(output, labels))\n",
    "    # corr is the number of equal elements between labels and output and thus the number of correctly classified \n",
    "    # labels for each sample \n",
    "    acc = corr/(N*C)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(output):\n",
    "    output[output > 0.5] = 1\n",
    "    output[output <= 0.5] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the loss computed for our case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use a simple classifier over a multi-label and multi-class dataset, your loss function is the sum/average over the binary classifier of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, in our case, we are trying to apply a simple classifier to solve a multi label classification problem. There are a few things that we should pay attention to in order to succeed in buiding our model for this specific task. In summary:\n",
    "\n",
    "- Number of nodes in the output layer matches the number of labels. \n",
    "- A sigmoid function should be applied for each node in the output layer. \n",
    "- A binary cross-entropy loss is the loss that should be used over each one of the labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training phase : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#from pygcn.utils import load_data, accuracy_sample, accuracy_sample_class\n",
    "from pygcn.models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BlogCatalog dataset...\n",
      "<_io.BufferedReader name='BlogCatalog/edges.csv'>\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(\"BlogCatalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Optimizer \n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.shape[1],\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gc1): GraphConvolution (10312 -> 16)\n",
       "  (gc2): GraphConvolution (16 -> 39)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HSAIN KENZA\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "output = model(features, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = np.sum([F.binary_cross_entropy_with_logits(output[idx_train][:,i], labels[idx_train][:,i]) for i in range(39)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.9938, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_sample_class(threshold(output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5373797960356005"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = np.sum([F.binary_cross_entropy_with_logits(output[idx_val][:,i], labels[idx_val][:,i]) for i in range(39)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val = accuracy_sample_class(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5412367673060138"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = np.sum([F.binary_cross_entropy_with_logits(output[idx_val][:,i], labels[idx_val][:,i]) for i in range(39)])\n",
    "acc_val = accuracy_sample_class(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5690039763359519"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(output.detach().numpy()[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = np.sum([F.binary_cross_entropy_with_logits(output[idx_train][:,i], labels[idx_train][:,i]) for i in range(39)])\n",
    "    acc_train = accuracy_sample_class(threshold(output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    print(np.sum(threshold(output.detach().numpy()[idx_train])>0))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        #model.eval()\n",
    "        #output = model(features, adj)\n",
    "\n",
    "    loss_val = np.sum([F.binary_cross_entropy_with_logits(output[idx_val][:,i], labels[idx_val][:,i]) for i in range(39)])\n",
    "    acc_val = accuracy_sample_class(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58335\n",
      "Epoch: 0001 loss_train: 36.8453 acc_train: 0.5547 loss_val: 36.8165 acc_val: 0.5572 time: 0.3541s\n",
      "56752\n",
      "Epoch: 0002 loss_train: 36.6824 acc_train: 0.5661 loss_val: 36.6596 acc_val: 0.5673 time: 0.3122s\n",
      "54927\n",
      "Epoch: 0003 loss_train: 36.4693 acc_train: 0.5786 loss_val: 36.4404 acc_val: 0.5796 time: 0.3271s\n",
      "53356\n",
      "Epoch: 0004 loss_train: 36.3028 acc_train: 0.5900 loss_val: 36.2703 acc_val: 0.5884 time: 0.4239s\n",
      "50579\n",
      "Epoch: 0005 loss_train: 36.0459 acc_train: 0.6094 loss_val: 36.0009 acc_val: 0.6096 time: 0.4338s\n",
      "44502\n",
      "Epoch: 0006 loss_train: 35.8528 acc_train: 0.6525 loss_val: 35.7934 acc_val: 0.6534 time: 0.3830s\n",
      "39081\n",
      "Epoch: 0007 loss_train: 35.6202 acc_train: 0.6914 loss_val: 35.5878 acc_val: 0.6929 time: 0.3271s\n",
      "36106\n",
      "Epoch: 0008 loss_train: 35.3937 acc_train: 0.7131 loss_val: 35.3487 acc_val: 0.7121 time: 0.3092s\n",
      "33550\n",
      "Epoch: 0009 loss_train: 35.1208 acc_train: 0.7310 loss_val: 35.0455 acc_val: 0.7256 time: 0.3293s\n",
      "31709\n",
      "Epoch: 0010 loss_train: 34.8532 acc_train: 0.7433 loss_val: 34.8378 acc_val: 0.7424 time: 0.3181s\n",
      "27313\n",
      "Epoch: 0011 loss_train: 34.6023 acc_train: 0.7731 loss_val: 34.5358 acc_val: 0.7713 time: 0.3710s\n",
      "23307\n",
      "Epoch: 0012 loss_train: 34.2750 acc_train: 0.8004 loss_val: 34.2214 acc_val: 0.7983 time: 0.3734s\n",
      "19022\n",
      "Epoch: 0013 loss_train: 34.0272 acc_train: 0.8301 loss_val: 34.0364 acc_val: 0.8303 time: 0.3451s\n",
      "16849\n",
      "Epoch: 0014 loss_train: 33.7122 acc_train: 0.8445 loss_val: 33.6345 acc_val: 0.8447 time: 0.3790s\n",
      "14639\n",
      "Epoch: 0015 loss_train: 33.4099 acc_train: 0.8602 loss_val: 33.3462 acc_val: 0.8601 time: 0.3371s\n",
      "13221\n",
      "Epoch: 0016 loss_train: 33.1996 acc_train: 0.8698 loss_val: 33.0827 acc_val: 0.8723 time: 0.3411s\n",
      "11067\n",
      "Epoch: 0017 loss_train: 32.7921 acc_train: 0.8850 loss_val: 32.7265 acc_val: 0.8851 time: 0.3541s\n",
      "9741\n",
      "Epoch: 0018 loss_train: 32.3668 acc_train: 0.8935 loss_val: 32.4131 acc_val: 0.8967 time: 0.3652s\n",
      "8501\n",
      "Epoch: 0019 loss_train: 32.2551 acc_train: 0.9023 loss_val: 32.1731 acc_val: 0.9044 time: 0.3820s\n",
      "7362\n",
      "Epoch: 0020 loss_train: 31.8411 acc_train: 0.9103 loss_val: 31.8457 acc_val: 0.9125 time: 0.3860s\n",
      "5857\n",
      "Epoch: 0021 loss_train: 31.4425 acc_train: 0.9209 loss_val: 31.4463 acc_val: 0.9213 time: 0.3301s\n",
      "2602\n",
      "Epoch: 0022 loss_train: 31.1657 acc_train: 0.9442 loss_val: 31.1584 acc_val: 0.9433 time: 0.3433s\n",
      "1158\n",
      "Epoch: 0023 loss_train: 30.9846 acc_train: 0.9548 loss_val: 30.8688 acc_val: 0.9548 time: 0.3572s\n",
      "613\n",
      "Epoch: 0024 loss_train: 30.6153 acc_train: 0.9587 loss_val: 30.5325 acc_val: 0.9588 time: 0.3391s\n",
      "456\n",
      "Epoch: 0025 loss_train: 30.3851 acc_train: 0.9600 loss_val: 30.3501 acc_val: 0.9609 time: 0.3281s\n",
      "240\n",
      "Epoch: 0026 loss_train: 30.1319 acc_train: 0.9614 loss_val: 30.0502 acc_val: 0.9617 time: 0.3341s\n",
      "209\n",
      "Epoch: 0027 loss_train: 29.8917 acc_train: 0.9617 loss_val: 29.9486 acc_val: 0.9619 time: 0.3261s\n",
      "150\n",
      "Epoch: 0028 loss_train: 29.5856 acc_train: 0.9620 loss_val: 29.4712 acc_val: 0.9629 time: 0.3690s\n",
      "107\n",
      "Epoch: 0029 loss_train: 29.5520 acc_train: 0.9624 loss_val: 29.5100 acc_val: 0.9624 time: 0.3979s\n",
      "116\n",
      "Epoch: 0030 loss_train: 29.3476 acc_train: 0.9623 loss_val: 29.3772 acc_val: 0.9624 time: 0.3441s\n",
      "101\n",
      "Epoch: 0031 loss_train: 29.2775 acc_train: 0.9624 loss_val: 29.1297 acc_val: 0.9628 time: 0.3460s\n",
      "113\n",
      "Epoch: 0032 loss_train: 28.9515 acc_train: 0.9623 loss_val: 28.9653 acc_val: 0.9631 time: 0.3461s\n",
      "95\n",
      "Epoch: 0033 loss_train: 28.9046 acc_train: 0.9625 loss_val: 28.8405 acc_val: 0.9633 time: 0.3470s\n",
      "109\n",
      "Epoch: 0034 loss_train: 28.7708 acc_train: 0.9624 loss_val: 28.6830 acc_val: 0.9633 time: 0.3620s\n",
      "63\n",
      "Epoch: 0035 loss_train: 28.5978 acc_train: 0.9627 loss_val: 28.6213 acc_val: 0.9629 time: 0.3610s\n",
      "126\n",
      "Epoch: 0036 loss_train: 28.6915 acc_train: 0.9623 loss_val: 28.5890 acc_val: 0.9633 time: 0.3570s\n",
      "114\n",
      "Epoch: 0037 loss_train: 28.4825 acc_train: 0.9623 loss_val: 28.4679 acc_val: 0.9627 time: 0.4159s\n",
      "64\n",
      "Epoch: 0038 loss_train: 28.4236 acc_train: 0.9627 loss_val: 28.3975 acc_val: 0.9631 time: 0.3745s\n",
      "67\n",
      "Epoch: 0039 loss_train: 28.3049 acc_train: 0.9627 loss_val: 28.3084 acc_val: 0.9633 time: 0.3311s\n",
      "32\n",
      "Epoch: 0040 loss_train: 28.2692 acc_train: 0.9629 loss_val: 28.2064 acc_val: 0.9636 time: 0.3780s\n",
      "65\n",
      "Epoch: 0041 loss_train: 28.1855 acc_train: 0.9627 loss_val: 28.1617 acc_val: 0.9632 time: 0.3643s\n",
      "56\n",
      "Epoch: 0042 loss_train: 28.1810 acc_train: 0.9627 loss_val: 28.1191 acc_val: 0.9632 time: 0.3690s\n",
      "60\n",
      "Epoch: 0043 loss_train: 28.1276 acc_train: 0.9628 loss_val: 28.1021 acc_val: 0.9632 time: 0.4119s\n",
      "70\n",
      "Epoch: 0044 loss_train: 28.0262 acc_train: 0.9627 loss_val: 27.9862 acc_val: 0.9633 time: 0.3883s\n",
      "41\n",
      "Epoch: 0045 loss_train: 28.0401 acc_train: 0.9629 loss_val: 28.0083 acc_val: 0.9633 time: 0.3450s\n",
      "25\n",
      "Epoch: 0046 loss_train: 27.8362 acc_train: 0.9630 loss_val: 27.8311 acc_val: 0.9635 time: 0.3491s\n",
      "37\n",
      "Epoch: 0047 loss_train: 27.9086 acc_train: 0.9629 loss_val: 27.8682 acc_val: 0.9634 time: 0.3531s\n",
      "39\n",
      "Epoch: 0048 loss_train: 27.8839 acc_train: 0.9629 loss_val: 27.8742 acc_val: 0.9634 time: 0.3979s\n",
      "14\n",
      "Epoch: 0049 loss_train: 27.8484 acc_train: 0.9631 loss_val: 27.8207 acc_val: 0.9634 time: 0.4189s\n",
      "26\n",
      "Epoch: 0050 loss_train: 27.7973 acc_train: 0.9630 loss_val: 27.7144 acc_val: 0.9636 time: 0.3970s\n",
      "27\n",
      "Epoch: 0051 loss_train: 27.7690 acc_train: 0.9629 loss_val: 27.7138 acc_val: 0.9634 time: 0.3660s\n",
      "38\n",
      "Epoch: 0052 loss_train: 27.7979 acc_train: 0.9629 loss_val: 27.7284 acc_val: 0.9635 time: 0.3850s\n",
      "53\n",
      "Epoch: 0053 loss_train: 27.7007 acc_train: 0.9628 loss_val: 27.6976 acc_val: 0.9634 time: 0.3890s\n",
      "30\n",
      "Epoch: 0054 loss_train: 27.7088 acc_train: 0.9629 loss_val: 27.6989 acc_val: 0.9634 time: 0.3962s\n",
      "17\n",
      "Epoch: 0055 loss_train: 27.6133 acc_train: 0.9630 loss_val: 27.6083 acc_val: 0.9634 time: 0.3640s\n",
      "34\n",
      "Epoch: 0056 loss_train: 27.6032 acc_train: 0.9629 loss_val: 27.5848 acc_val: 0.9635 time: 0.3690s\n",
      "33\n",
      "Epoch: 0057 loss_train: 27.6422 acc_train: 0.9630 loss_val: 27.6416 acc_val: 0.9635 time: 0.3833s\n",
      "24\n",
      "Epoch: 0058 loss_train: 27.5431 acc_train: 0.9630 loss_val: 27.5141 acc_val: 0.9636 time: 0.4155s\n",
      "44\n",
      "Epoch: 0059 loss_train: 27.6240 acc_train: 0.9629 loss_val: 27.6013 acc_val: 0.9632 time: 0.3927s\n",
      "17\n",
      "Epoch: 0060 loss_train: 27.6382 acc_train: 0.9630 loss_val: 27.5705 acc_val: 0.9635 time: 0.3851s\n",
      "12\n",
      "Epoch: 0061 loss_train: 27.5619 acc_train: 0.9631 loss_val: 27.5771 acc_val: 0.9635 time: 0.3875s\n",
      "31\n",
      "Epoch: 0062 loss_train: 27.5279 acc_train: 0.9630 loss_val: 27.5010 acc_val: 0.9635 time: 0.3727s\n",
      "42\n",
      "Epoch: 0063 loss_train: 27.5606 acc_train: 0.9628 loss_val: 27.5348 acc_val: 0.9635 time: 0.4000s\n",
      "20\n",
      "Epoch: 0064 loss_train: 27.5055 acc_train: 0.9630 loss_val: 27.5482 acc_val: 0.9635 time: 0.3879s\n",
      "24\n",
      "Epoch: 0065 loss_train: 27.5641 acc_train: 0.9630 loss_val: 27.5408 acc_val: 0.9635 time: 0.3822s\n",
      "25\n",
      "Epoch: 0066 loss_train: 27.4771 acc_train: 0.9630 loss_val: 27.4683 acc_val: 0.9636 time: 0.3939s\n",
      "34\n",
      "Epoch: 0067 loss_train: 27.5026 acc_train: 0.9629 loss_val: 27.5149 acc_val: 0.9634 time: 0.4069s\n",
      "51\n",
      "Epoch: 0068 loss_train: 27.4782 acc_train: 0.9628 loss_val: 27.4508 acc_val: 0.9635 time: 0.3830s\n",
      "33\n",
      "Epoch: 0069 loss_train: 27.4787 acc_train: 0.9629 loss_val: 27.4452 acc_val: 0.9635 time: 0.3730s\n",
      "42\n",
      "Epoch: 0070 loss_train: 27.4282 acc_train: 0.9629 loss_val: 27.3851 acc_val: 0.9637 time: 0.4039s\n",
      "50\n",
      "Epoch: 0071 loss_train: 27.4494 acc_train: 0.9628 loss_val: 27.3927 acc_val: 0.9636 time: 0.3712s\n",
      "30\n",
      "Epoch: 0072 loss_train: 27.4217 acc_train: 0.9629 loss_val: 27.3988 acc_val: 0.9636 time: 0.4060s\n",
      "24\n",
      "Epoch: 0073 loss_train: 27.4683 acc_train: 0.9630 loss_val: 27.4062 acc_val: 0.9637 time: 0.4222s\n",
      "27\n",
      "Epoch: 0074 loss_train: 27.4238 acc_train: 0.9630 loss_val: 27.3945 acc_val: 0.9636 time: 0.3984s\n",
      "71\n",
      "Epoch: 0075 loss_train: 27.4266 acc_train: 0.9626 loss_val: 27.3977 acc_val: 0.9637 time: 0.4109s\n",
      "48\n",
      "Epoch: 0076 loss_train: 27.4239 acc_train: 0.9628 loss_val: 27.4048 acc_val: 0.9632 time: 0.3984s\n",
      "16\n",
      "Epoch: 0077 loss_train: 27.3430 acc_train: 0.9630 loss_val: 27.3364 acc_val: 0.9635 time: 0.4070s\n",
      "9\n",
      "Epoch: 0078 loss_train: 27.3570 acc_train: 0.9631 loss_val: 27.3757 acc_val: 0.9636 time: 0.3829s\n",
      "28\n",
      "Epoch: 0079 loss_train: 27.4177 acc_train: 0.9630 loss_val: 27.3848 acc_val: 0.9636 time: 0.3613s\n",
      "13\n",
      "Epoch: 0080 loss_train: 27.3488 acc_train: 0.9630 loss_val: 27.3330 acc_val: 0.9635 time: 0.4099s\n",
      "35\n",
      "Epoch: 0081 loss_train: 27.3590 acc_train: 0.9629 loss_val: 27.3664 acc_val: 0.9634 time: 0.3920s\n",
      "30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0082 loss_train: 27.3460 acc_train: 0.9629 loss_val: 27.3163 acc_val: 0.9637 time: 0.3820s\n",
      "12\n",
      "Epoch: 0083 loss_train: 27.3335 acc_train: 0.9631 loss_val: 27.3385 acc_val: 0.9635 time: 0.4059s\n",
      "36\n",
      "Epoch: 0084 loss_train: 27.3367 acc_train: 0.9629 loss_val: 27.3591 acc_val: 0.9634 time: 0.3920s\n",
      "21\n",
      "Epoch: 0085 loss_train: 27.3515 acc_train: 0.9630 loss_val: 27.3487 acc_val: 0.9637 time: 0.3780s\n",
      "26\n",
      "Epoch: 0086 loss_train: 27.3716 acc_train: 0.9630 loss_val: 27.3242 acc_val: 0.9635 time: 0.3790s\n",
      "26\n",
      "Epoch: 0087 loss_train: 27.3492 acc_train: 0.9630 loss_val: 27.3288 acc_val: 0.9636 time: 0.3710s\n",
      "16\n",
      "Epoch: 0088 loss_train: 27.3226 acc_train: 0.9631 loss_val: 27.3313 acc_val: 0.9635 time: 0.4338s\n",
      "15\n",
      "Epoch: 0089 loss_train: 27.3120 acc_train: 0.9630 loss_val: 27.3111 acc_val: 0.9636 time: 0.4628s\n",
      "27\n",
      "Epoch: 0090 loss_train: 27.3138 acc_train: 0.9630 loss_val: 27.2898 acc_val: 0.9635 time: 0.4819s\n",
      "38\n",
      "Epoch: 0091 loss_train: 27.3427 acc_train: 0.9629 loss_val: 27.3382 acc_val: 0.9635 time: 0.4448s\n",
      "22\n",
      "Epoch: 0092 loss_train: 27.3149 acc_train: 0.9630 loss_val: 27.3259 acc_val: 0.9635 time: 0.4009s\n",
      "41\n",
      "Epoch: 0093 loss_train: 27.3332 acc_train: 0.9629 loss_val: 27.3273 acc_val: 0.9636 time: 0.3772s\n",
      "6\n",
      "Epoch: 0094 loss_train: 27.2844 acc_train: 0.9631 loss_val: 27.2747 acc_val: 0.9635 time: 0.3969s\n",
      "24\n",
      "Epoch: 0095 loss_train: 27.2659 acc_train: 0.9630 loss_val: 27.2888 acc_val: 0.9637 time: 0.3949s\n",
      "15\n",
      "Epoch: 0096 loss_train: 27.3081 acc_train: 0.9630 loss_val: 27.3054 acc_val: 0.9634 time: 0.3720s\n",
      "16\n",
      "Epoch: 0097 loss_train: 27.2890 acc_train: 0.9630 loss_val: 27.2646 acc_val: 0.9637 time: 0.3911s\n",
      "6\n",
      "Epoch: 0098 loss_train: 27.2974 acc_train: 0.9631 loss_val: 27.2662 acc_val: 0.9636 time: 0.4239s\n",
      "6\n",
      "Epoch: 0099 loss_train: 27.2755 acc_train: 0.9631 loss_val: 27.2938 acc_val: 0.9635 time: 0.4079s\n",
      "19\n",
      "Epoch: 0100 loss_train: 27.2679 acc_train: 0.9630 loss_val: 27.2617 acc_val: 0.9636 time: 0.5047s\n",
      "11\n",
      "Epoch: 0101 loss_train: 27.2877 acc_train: 0.9631 loss_val: 27.2830 acc_val: 0.9636 time: 0.4281s\n",
      "36\n",
      "Epoch: 0102 loss_train: 27.2723 acc_train: 0.9629 loss_val: 27.2759 acc_val: 0.9636 time: 0.4787s\n",
      "21\n",
      "Epoch: 0103 loss_train: 27.2308 acc_train: 0.9630 loss_val: 27.2564 acc_val: 0.9637 time: 0.4568s\n",
      "5\n",
      "Epoch: 0104 loss_train: 27.2267 acc_train: 0.9631 loss_val: 27.2304 acc_val: 0.9637 time: 0.4209s\n",
      "7\n",
      "Epoch: 0105 loss_train: 27.2666 acc_train: 0.9631 loss_val: 27.2434 acc_val: 0.9636 time: 0.4099s\n",
      "15\n",
      "Epoch: 0106 loss_train: 27.2740 acc_train: 0.9630 loss_val: 27.2678 acc_val: 0.9637 time: 0.3900s\n",
      "10\n",
      "Epoch: 0107 loss_train: 27.2605 acc_train: 0.9631 loss_val: 27.2653 acc_val: 0.9636 time: 0.4009s\n",
      "19\n",
      "Epoch: 0108 loss_train: 27.2455 acc_train: 0.9630 loss_val: 27.2168 acc_val: 0.9637 time: 0.3980s\n",
      "20\n",
      "Epoch: 0109 loss_train: 27.2375 acc_train: 0.9630 loss_val: 27.2341 acc_val: 0.9637 time: 0.4089s\n",
      "25\n",
      "Epoch: 0110 loss_train: 27.2419 acc_train: 0.9630 loss_val: 27.2563 acc_val: 0.9636 time: 0.3939s\n",
      "1\n",
      "Epoch: 0111 loss_train: 27.2213 acc_train: 0.9631 loss_val: 27.2306 acc_val: 0.9637 time: 0.4161s\n",
      "4\n",
      "Epoch: 0112 loss_train: 27.2337 acc_train: 0.9631 loss_val: 27.2415 acc_val: 0.9636 time: 0.3800s\n",
      "2\n",
      "Epoch: 0113 loss_train: 27.2235 acc_train: 0.9631 loss_val: 27.2320 acc_val: 0.9635 time: 0.3929s\n",
      "34\n",
      "Epoch: 0114 loss_train: 27.2848 acc_train: 0.9629 loss_val: 27.2407 acc_val: 0.9635 time: 0.4050s\n",
      "34\n",
      "Epoch: 0115 loss_train: 27.2630 acc_train: 0.9630 loss_val: 27.2515 acc_val: 0.9636 time: 0.3959s\n",
      "26\n",
      "Epoch: 0116 loss_train: 27.2494 acc_train: 0.9630 loss_val: 27.2317 acc_val: 0.9636 time: 0.3860s\n",
      "17\n",
      "Epoch: 0117 loss_train: 27.2887 acc_train: 0.9630 loss_val: 27.2408 acc_val: 0.9636 time: 0.4141s\n",
      "30\n",
      "Epoch: 0118 loss_train: 27.2350 acc_train: 0.9629 loss_val: 27.2280 acc_val: 0.9637 time: 0.4109s\n",
      "15\n",
      "Epoch: 0119 loss_train: 27.2690 acc_train: 0.9631 loss_val: 27.2309 acc_val: 0.9636 time: 0.3999s\n",
      "10\n",
      "Epoch: 0120 loss_train: 27.2214 acc_train: 0.9631 loss_val: 27.2224 acc_val: 0.9636 time: 0.4239s\n",
      "21\n",
      "Epoch: 0121 loss_train: 27.2088 acc_train: 0.9630 loss_val: 27.2082 acc_val: 0.9637 time: 0.3870s\n",
      "5\n",
      "Epoch: 0122 loss_train: 27.1827 acc_train: 0.9631 loss_val: 27.2055 acc_val: 0.9637 time: 0.3960s\n",
      "59\n",
      "Epoch: 0123 loss_train: 27.2110 acc_train: 0.9627 loss_val: 27.2009 acc_val: 0.9637 time: 0.4229s\n",
      "38\n",
      "Epoch: 0124 loss_train: 27.2167 acc_train: 0.9629 loss_val: 27.2281 acc_val: 0.9636 time: 0.3851s\n",
      "15\n",
      "Epoch: 0125 loss_train: 27.2318 acc_train: 0.9630 loss_val: 27.2333 acc_val: 0.9636 time: 0.4089s\n",
      "8\n",
      "Epoch: 0126 loss_train: 27.2255 acc_train: 0.9631 loss_val: 27.2063 acc_val: 0.9637 time: 0.4522s\n",
      "45\n",
      "Epoch: 0127 loss_train: 27.2074 acc_train: 0.9628 loss_val: 27.2000 acc_val: 0.9636 time: 0.5136s\n",
      "33\n",
      "Epoch: 0128 loss_train: 27.2268 acc_train: 0.9629 loss_val: 27.2119 acc_val: 0.9636 time: 0.4879s\n",
      "5\n",
      "Epoch: 0129 loss_train: 27.1905 acc_train: 0.9631 loss_val: 27.2147 acc_val: 0.9636 time: 0.4209s\n",
      "21\n",
      "Epoch: 0130 loss_train: 27.1982 acc_train: 0.9630 loss_val: 27.1895 acc_val: 0.9637 time: 0.4228s\n",
      "11\n",
      "Epoch: 0131 loss_train: 27.1842 acc_train: 0.9631 loss_val: 27.1836 acc_val: 0.9637 time: 0.4279s\n",
      "17\n",
      "Epoch: 0132 loss_train: 27.2250 acc_train: 0.9630 loss_val: 27.2117 acc_val: 0.9636 time: 0.3995s\n",
      "17\n",
      "Epoch: 0133 loss_train: 27.1907 acc_train: 0.9630 loss_val: 27.1951 acc_val: 0.9637 time: 0.3850s\n",
      "29\n",
      "Epoch: 0134 loss_train: 27.1930 acc_train: 0.9629 loss_val: 27.1933 acc_val: 0.9636 time: 0.4209s\n",
      "34\n",
      "Epoch: 0135 loss_train: 27.2195 acc_train: 0.9629 loss_val: 27.2008 acc_val: 0.9637 time: 0.3949s\n",
      "16\n",
      "Epoch: 0136 loss_train: 27.1709 acc_train: 0.9630 loss_val: 27.1775 acc_val: 0.9637 time: 0.3999s\n",
      "2\n",
      "Epoch: 0137 loss_train: 27.1718 acc_train: 0.9631 loss_val: 27.1863 acc_val: 0.9637 time: 0.3799s\n",
      "2\n",
      "Epoch: 0138 loss_train: 27.1603 acc_train: 0.9631 loss_val: 27.1567 acc_val: 0.9637 time: 0.4020s\n",
      "11\n",
      "Epoch: 0139 loss_train: 27.2119 acc_train: 0.9631 loss_val: 27.1950 acc_val: 0.9637 time: 0.4019s\n",
      "7\n",
      "Epoch: 0140 loss_train: 27.2118 acc_train: 0.9631 loss_val: 27.1784 acc_val: 0.9637 time: 0.4478s\n",
      "21\n",
      "Epoch: 0141 loss_train: 27.1756 acc_train: 0.9630 loss_val: 27.1712 acc_val: 0.9636 time: 0.3969s\n",
      "6\n",
      "Epoch: 0142 loss_train: 27.1618 acc_train: 0.9631 loss_val: 27.1901 acc_val: 0.9636 time: 0.4179s\n",
      "4\n",
      "Epoch: 0143 loss_train: 27.1615 acc_train: 0.9631 loss_val: 27.1719 acc_val: 0.9637 time: 0.4318s\n",
      "3\n",
      "Epoch: 0144 loss_train: 27.1693 acc_train: 0.9631 loss_val: 27.1680 acc_val: 0.9636 time: 0.4109s\n",
      "4\n",
      "Epoch: 0145 loss_train: 27.1673 acc_train: 0.9631 loss_val: 27.1736 acc_val: 0.9637 time: 0.4149s\n",
      "22\n",
      "Epoch: 0146 loss_train: 27.1882 acc_train: 0.9630 loss_val: 27.1797 acc_val: 0.9637 time: 0.3830s\n",
      "32\n",
      "Epoch: 0147 loss_train: 27.1905 acc_train: 0.9629 loss_val: 27.1781 acc_val: 0.9636 time: 0.4159s\n",
      "21\n",
      "Epoch: 0148 loss_train: 27.1825 acc_train: 0.9630 loss_val: 27.1615 acc_val: 0.9637 time: 0.4019s\n",
      "25\n",
      "Epoch: 0149 loss_train: 27.2182 acc_train: 0.9630 loss_val: 27.1807 acc_val: 0.9637 time: 0.3959s\n",
      "0\n",
      "Epoch: 0150 loss_train: 27.1785 acc_train: 0.9631 loss_val: 27.1790 acc_val: 0.9636 time: 0.3999s\n",
      "38\n",
      "Epoch: 0151 loss_train: 27.1925 acc_train: 0.9629 loss_val: 27.1999 acc_val: 0.9635 time: 0.4328s\n",
      "14\n",
      "Epoch: 0152 loss_train: 27.1864 acc_train: 0.9630 loss_val: 27.1919 acc_val: 0.9637 time: 0.3870s\n",
      "11\n",
      "Epoch: 0153 loss_train: 27.1952 acc_train: 0.9631 loss_val: 27.1586 acc_val: 0.9637 time: 0.4229s\n",
      "10\n",
      "Epoch: 0154 loss_train: 27.1848 acc_train: 0.9631 loss_val: 27.2170 acc_val: 0.9636 time: 0.4169s\n",
      "9\n",
      "Epoch: 0155 loss_train: 27.1837 acc_train: 0.9631 loss_val: 27.1905 acc_val: 0.9637 time: 0.4308s\n",
      "11\n",
      "Epoch: 0156 loss_train: 27.2010 acc_train: 0.9631 loss_val: 27.1918 acc_val: 0.9637 time: 0.4338s\n",
      "6\n",
      "Epoch: 0157 loss_train: 27.1385 acc_train: 0.9631 loss_val: 27.1530 acc_val: 0.9636 time: 0.3989s\n",
      "13\n",
      "Epoch: 0158 loss_train: 27.1788 acc_train: 0.9630 loss_val: 27.1523 acc_val: 0.9635 time: 0.4099s\n",
      "3\n",
      "Epoch: 0159 loss_train: 27.1838 acc_train: 0.9631 loss_val: 27.1476 acc_val: 0.9636 time: 0.4219s\n",
      "8\n",
      "Epoch: 0160 loss_train: 27.1685 acc_train: 0.9631 loss_val: 27.1667 acc_val: 0.9636 time: 0.4149s\n",
      "16\n",
      "Epoch: 0161 loss_train: 27.1764 acc_train: 0.9630 loss_val: 27.1689 acc_val: 0.9636 time: 0.4114s\n",
      "18\n",
      "Epoch: 0162 loss_train: 27.1865 acc_train: 0.9630 loss_val: 27.1858 acc_val: 0.9637 time: 0.4239s\n",
      "10\n",
      "Epoch: 0163 loss_train: 27.1722 acc_train: 0.9631 loss_val: 27.1564 acc_val: 0.9637 time: 0.4259s\n",
      "33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0164 loss_train: 27.1752 acc_train: 0.9629 loss_val: 27.1629 acc_val: 0.9634 time: 0.4398s\n",
      "3\n",
      "Epoch: 0165 loss_train: 27.1674 acc_train: 0.9631 loss_val: 27.1483 acc_val: 0.9636 time: 0.5037s\n",
      "5\n",
      "Epoch: 0166 loss_train: 27.1669 acc_train: 0.9631 loss_val: 27.1642 acc_val: 0.9636 time: 0.4957s\n",
      "23\n",
      "Epoch: 0167 loss_train: 27.1517 acc_train: 0.9630 loss_val: 27.1566 acc_val: 0.9637 time: 0.4379s\n",
      "12\n",
      "Epoch: 0168 loss_train: 27.1656 acc_train: 0.9631 loss_val: 27.1497 acc_val: 0.9637 time: 0.4149s\n",
      "11\n",
      "Epoch: 0169 loss_train: 27.1330 acc_train: 0.9631 loss_val: 27.1175 acc_val: 0.9637 time: 0.4109s\n",
      "12\n",
      "Epoch: 0170 loss_train: 27.1383 acc_train: 0.9631 loss_val: 27.1629 acc_val: 0.9636 time: 0.4209s\n",
      "6\n",
      "Epoch: 0171 loss_train: 27.1268 acc_train: 0.9631 loss_val: 27.1285 acc_val: 0.9637 time: 0.4289s\n",
      "8\n",
      "Epoch: 0172 loss_train: 27.1852 acc_train: 0.9631 loss_val: 27.1518 acc_val: 0.9637 time: 0.4199s\n",
      "1\n",
      "Epoch: 0173 loss_train: 27.1531 acc_train: 0.9631 loss_val: 27.1588 acc_val: 0.9637 time: 0.4478s\n",
      "2\n",
      "Epoch: 0174 loss_train: 27.1762 acc_train: 0.9631 loss_val: 27.1670 acc_val: 0.9637 time: 0.4409s\n",
      "2\n",
      "Epoch: 0175 loss_train: 27.1397 acc_train: 0.9631 loss_val: 27.1342 acc_val: 0.9637 time: 0.4039s\n",
      "16\n",
      "Epoch: 0176 loss_train: 27.1614 acc_train: 0.9630 loss_val: 27.1420 acc_val: 0.9637 time: 0.4309s\n",
      "11\n",
      "Epoch: 0177 loss_train: 27.1690 acc_train: 0.9631 loss_val: 27.1793 acc_val: 0.9635 time: 0.3900s\n",
      "28\n",
      "Epoch: 0178 loss_train: 27.1443 acc_train: 0.9629 loss_val: 27.1328 acc_val: 0.9637 time: 0.4468s\n",
      "10\n",
      "Epoch: 0179 loss_train: 27.1829 acc_train: 0.9631 loss_val: 27.1440 acc_val: 0.9637 time: 0.4358s\n",
      "2\n",
      "Epoch: 0180 loss_train: 27.1575 acc_train: 0.9631 loss_val: 27.1445 acc_val: 0.9637 time: 0.4161s\n",
      "22\n",
      "Epoch: 0181 loss_train: 27.1404 acc_train: 0.9630 loss_val: 27.1299 acc_val: 0.9637 time: 0.4301s\n",
      "9\n",
      "Epoch: 0182 loss_train: 27.1472 acc_train: 0.9631 loss_val: 27.1468 acc_val: 0.9636 time: 0.4020s\n",
      "0\n",
      "Epoch: 0183 loss_train: 27.1131 acc_train: 0.9631 loss_val: 27.1192 acc_val: 0.9637 time: 0.4268s\n",
      "2\n",
      "Epoch: 0184 loss_train: 27.1472 acc_train: 0.9631 loss_val: 27.1635 acc_val: 0.9636 time: 0.4089s\n",
      "1\n",
      "Epoch: 0185 loss_train: 27.1711 acc_train: 0.9631 loss_val: 27.1519 acc_val: 0.9637 time: 0.4079s\n",
      "1\n",
      "Epoch: 0186 loss_train: 27.1344 acc_train: 0.9631 loss_val: 27.1434 acc_val: 0.9637 time: 0.4010s\n",
      "5\n",
      "Epoch: 0187 loss_train: 27.1304 acc_train: 0.9631 loss_val: 27.1246 acc_val: 0.9637 time: 0.4338s\n",
      "0\n",
      "Epoch: 0188 loss_train: 27.1450 acc_train: 0.9631 loss_val: 27.1404 acc_val: 0.9636 time: 0.4488s\n",
      "9\n",
      "Epoch: 0189 loss_train: 27.1557 acc_train: 0.9631 loss_val: 27.1463 acc_val: 0.9637 time: 0.4209s\n",
      "5\n",
      "Epoch: 0190 loss_train: 27.1540 acc_train: 0.9631 loss_val: 27.1646 acc_val: 0.9637 time: 0.3890s\n",
      "10\n",
      "Epoch: 0191 loss_train: 27.1681 acc_train: 0.9631 loss_val: 27.1514 acc_val: 0.9636 time: 0.4199s\n",
      "1\n",
      "Epoch: 0192 loss_train: 27.1275 acc_train: 0.9631 loss_val: 27.1244 acc_val: 0.9637 time: 0.4378s\n",
      "22\n",
      "Epoch: 0193 loss_train: 27.1658 acc_train: 0.9630 loss_val: 27.1833 acc_val: 0.9636 time: 0.4408s\n",
      "6\n",
      "Epoch: 0194 loss_train: 27.1368 acc_train: 0.9631 loss_val: 27.1367 acc_val: 0.9637 time: 0.4039s\n",
      "15\n",
      "Epoch: 0195 loss_train: 27.1579 acc_train: 0.9631 loss_val: 27.1382 acc_val: 0.9637 time: 0.4159s\n",
      "5\n",
      "Epoch: 0196 loss_train: 27.1244 acc_train: 0.9631 loss_val: 27.1190 acc_val: 0.9637 time: 0.4069s\n",
      "2\n",
      "Epoch: 0197 loss_train: 27.1341 acc_train: 0.9631 loss_val: 27.1214 acc_val: 0.9637 time: 0.4169s\n",
      "5\n",
      "Epoch: 0198 loss_train: 27.1340 acc_train: 0.9631 loss_val: 27.1178 acc_val: 0.9637 time: 0.4299s\n",
      "4\n",
      "Epoch: 0199 loss_train: 27.1454 acc_train: 0.9631 loss_val: 27.1579 acc_val: 0.9636 time: 0.4269s\n",
      "10\n",
      "Epoch: 0200 loss_train: 27.1239 acc_train: 0.9631 loss_val: 27.1201 acc_val: 0.9637 time: 0.4378s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 80.8307s\n"
     ]
    }
   ],
   "source": [
    "# without taking into consideration the imbalance \n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = np.sum([F.binary_cross_entropy_with_logits(output[idx_test][:,i], labels[idx_test][:,i]) for i in range(39)])\n",
    "    acc_test = accuracy_sample_class(threshold(output.detach().numpy()[idx_test]), labels.detach().numpy()[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 27.0957 accuracy= 0.9651\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with the imbalance of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider each label with its own binary classification problem, the dataset is clearly imbalanced in each problem with a favorable position for the label 0 that represents the absence of the label for the corresponding node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt : use the weight parameter in the binary_cross_entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(c):\n",
    "    '''\n",
    "    c is a set of labels represented by 0 and 1\n",
    "    '''\n",
    "    n_0 = np. count_nonzero(c == 0)\n",
    "    n_1 = np.count_nonzero(c == 1)\n",
    "    N = c.shape[0]\n",
    "    res = torch.tensor([1/n_0,1/n_1])*N\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Optimizer \n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.shape[1],\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    #weight = torch.tensor([2.0,1.0])\n",
    "    loss_train = np.sum([(F.binary_cross_entropy_with_logits(output[idx_train][:,i], labels[idx_train][:,i])*weight(labels[idx_train][:,i])).mean() for i in range(39)])\n",
    "    acc_train = accuracy_sample_class(threshold(output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        #model.eval()\n",
    "        #output = model(features, adj)\n",
    "\n",
    "    loss_val = np.sum([(F.binary_cross_entropy_with_logits(output[idx_val][:,i], labels[idx_val][:,i])*weight(labels[idx_val][:,i])).mean() for i in range(39)])\n",
    "    acc_val = accuracy_sample_class(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2551.3767 acc_train: 0.3933 loss_val: 3165.1851 acc_val: 0.4094 time: 0.6459s\n",
      "Epoch: 0002 loss_train: 1916.8715 acc_train: 0.6995 loss_val: 2342.1960 acc_val: 0.6960 time: 0.6266s\n",
      "Epoch: 0003 loss_train: 1796.5952 acc_train: 0.8862 loss_val: 2227.0566 acc_val: 0.8864 time: 0.6153s\n",
      "Epoch: 0004 loss_train: 1781.4299 acc_train: 0.9621 loss_val: 2210.3606 acc_val: 0.9627 time: 0.6274s\n",
      "Epoch: 0005 loss_train: 1779.4535 acc_train: 0.9625 loss_val: 2209.1763 acc_val: 0.9630 time: 0.5481s\n",
      "Epoch: 0006 loss_train: 1776.4615 acc_train: 0.9627 loss_val: 2206.1196 acc_val: 0.9632 time: 0.6083s\n",
      "Epoch: 0007 loss_train: 1775.4109 acc_train: 0.9626 loss_val: 2204.6379 acc_val: 0.9633 time: 0.6054s\n",
      "Epoch: 0008 loss_train: 1775.0363 acc_train: 0.9624 loss_val: 2205.0432 acc_val: 0.9630 time: 0.6421s\n",
      "Epoch: 0009 loss_train: 1774.9130 acc_train: 0.9629 loss_val: 2203.7161 acc_val: 0.9635 time: 0.6489s\n",
      "Epoch: 0010 loss_train: 1775.4124 acc_train: 0.9625 loss_val: 2204.6025 acc_val: 0.9630 time: 0.6587s\n",
      "Epoch: 0011 loss_train: 1775.2560 acc_train: 0.9627 loss_val: 2204.9309 acc_val: 0.9629 time: 0.5894s\n",
      "Epoch: 0012 loss_train: 1773.8795 acc_train: 0.9628 loss_val: 2203.3569 acc_val: 0.9634 time: 0.6285s\n",
      "Epoch: 0013 loss_train: 1774.1289 acc_train: 0.9625 loss_val: 2203.4565 acc_val: 0.9632 time: 0.6186s\n",
      "Epoch: 0014 loss_train: 1773.8835 acc_train: 0.9628 loss_val: 2203.0347 acc_val: 0.9633 time: 0.6414s\n",
      "Epoch: 0015 loss_train: 1773.7302 acc_train: 0.9627 loss_val: 2203.5432 acc_val: 0.9632 time: 0.6259s\n",
      "Epoch: 0016 loss_train: 1772.9753 acc_train: 0.9630 loss_val: 2202.6880 acc_val: 0.9634 time: 0.6540s\n",
      "Epoch: 0017 loss_train: 1774.0746 acc_train: 0.9628 loss_val: 2202.9824 acc_val: 0.9633 time: 0.6330s\n",
      "Epoch: 0018 loss_train: 1773.4172 acc_train: 0.9628 loss_val: 2202.9016 acc_val: 0.9633 time: 0.6078s\n",
      "Epoch: 0019 loss_train: 1773.8468 acc_train: 0.9625 loss_val: 2203.2974 acc_val: 0.9634 time: 0.6061s\n",
      "Epoch: 0020 loss_train: 1774.9988 acc_train: 0.9622 loss_val: 2203.7051 acc_val: 0.9630 time: 0.5833s\n",
      "Epoch: 0021 loss_train: 1773.6776 acc_train: 0.9626 loss_val: 2202.8215 acc_val: 0.9632 time: 0.6738s\n",
      "Epoch: 0022 loss_train: 1773.1479 acc_train: 0.9624 loss_val: 2202.5845 acc_val: 0.9633 time: 0.6075s\n",
      "Epoch: 0023 loss_train: 1773.3210 acc_train: 0.9628 loss_val: 2202.3137 acc_val: 0.9635 time: 0.6045s\n",
      "Epoch: 0024 loss_train: 1773.0148 acc_train: 0.9627 loss_val: 2202.5566 acc_val: 0.9633 time: 0.6295s\n",
      "Epoch: 0025 loss_train: 1773.5548 acc_train: 0.9626 loss_val: 2202.0181 acc_val: 0.9636 time: 0.6853s\n",
      "Epoch: 0026 loss_train: 1772.7527 acc_train: 0.9629 loss_val: 2202.3096 acc_val: 0.9635 time: 0.6113s\n",
      "Epoch: 0027 loss_train: 1772.9664 acc_train: 0.9628 loss_val: 2202.2681 acc_val: 0.9633 time: 0.5923s\n",
      "Epoch: 0028 loss_train: 1773.0292 acc_train: 0.9627 loss_val: 2202.8508 acc_val: 0.9630 time: 0.5834s\n",
      "Epoch: 0029 loss_train: 1772.7678 acc_train: 0.9629 loss_val: 2201.7185 acc_val: 0.9636 time: 0.6440s\n",
      "Epoch: 0030 loss_train: 1773.4053 acc_train: 0.9626 loss_val: 2202.2310 acc_val: 0.9635 time: 0.6172s\n",
      "Epoch: 0031 loss_train: 1772.4885 acc_train: 0.9630 loss_val: 2202.6179 acc_val: 0.9633 time: 0.6162s\n",
      "Epoch: 0032 loss_train: 1773.0140 acc_train: 0.9627 loss_val: 2201.9849 acc_val: 0.9634 time: 0.6463s\n",
      "Epoch: 0033 loss_train: 1773.0746 acc_train: 0.9628 loss_val: 2201.8032 acc_val: 0.9636 time: 0.6640s\n",
      "Epoch: 0034 loss_train: 1773.2146 acc_train: 0.9623 loss_val: 2202.5825 acc_val: 0.9630 time: 0.7252s\n",
      "Epoch: 0035 loss_train: 1773.0898 acc_train: 0.9628 loss_val: 2203.1816 acc_val: 0.9629 time: 0.7217s\n",
      "Epoch: 0036 loss_train: 1773.0811 acc_train: 0.9626 loss_val: 2204.1787 acc_val: 0.9625 time: 0.7239s\n",
      "Epoch: 0037 loss_train: 1773.2836 acc_train: 0.9625 loss_val: 2202.3943 acc_val: 0.9631 time: 0.6531s\n",
      "Epoch: 0038 loss_train: 1772.3171 acc_train: 0.9630 loss_val: 2202.1060 acc_val: 0.9633 time: 0.7030s\n",
      "Epoch: 0039 loss_train: 1772.6296 acc_train: 0.9628 loss_val: 2202.0964 acc_val: 0.9635 time: 0.6151s\n",
      "Epoch: 0040 loss_train: 1773.0605 acc_train: 0.9626 loss_val: 2202.9758 acc_val: 0.9629 time: 0.6073s\n",
      "Epoch: 0041 loss_train: 1773.1642 acc_train: 0.9626 loss_val: 2202.0186 acc_val: 0.9632 time: 0.5693s\n",
      "Epoch: 0042 loss_train: 1773.6406 acc_train: 0.9624 loss_val: 2203.0505 acc_val: 0.9629 time: 0.6326s\n",
      "Epoch: 0043 loss_train: 1773.3179 acc_train: 0.9624 loss_val: 2202.1692 acc_val: 0.9633 time: 0.7249s\n",
      "Epoch: 0044 loss_train: 1772.4744 acc_train: 0.9630 loss_val: 2202.0725 acc_val: 0.9635 time: 0.6309s\n",
      "Epoch: 0045 loss_train: 1772.7595 acc_train: 0.9629 loss_val: 2201.9800 acc_val: 0.9634 time: 0.6435s\n",
      "Epoch: 0046 loss_train: 1772.8961 acc_train: 0.9628 loss_val: 2202.0664 acc_val: 0.9632 time: 0.6689s\n",
      "Epoch: 0047 loss_train: 1772.9059 acc_train: 0.9626 loss_val: 2202.0066 acc_val: 0.9634 time: 0.6824s\n",
      "Epoch: 0048 loss_train: 1772.5931 acc_train: 0.9625 loss_val: 2201.6189 acc_val: 0.9634 time: 0.6161s\n",
      "Epoch: 0049 loss_train: 1772.9761 acc_train: 0.9626 loss_val: 2202.1238 acc_val: 0.9633 time: 0.6466s\n",
      "Epoch: 0050 loss_train: 1772.6775 acc_train: 0.9627 loss_val: 2201.9949 acc_val: 0.9636 time: 0.6030s\n",
      "Epoch: 0051 loss_train: 1772.6376 acc_train: 0.9628 loss_val: 2202.6079 acc_val: 0.9630 time: 0.6585s\n",
      "Epoch: 0052 loss_train: 1772.8601 acc_train: 0.9627 loss_val: 2201.9551 acc_val: 0.9634 time: 0.5999s\n",
      "Epoch: 0053 loss_train: 1772.8634 acc_train: 0.9627 loss_val: 2201.7510 acc_val: 0.9636 time: 0.6617s\n",
      "Epoch: 0054 loss_train: 1772.4381 acc_train: 0.9628 loss_val: 2201.7410 acc_val: 0.9633 time: 0.7015s\n",
      "Epoch: 0055 loss_train: 1772.8425 acc_train: 0.9626 loss_val: 2201.9756 acc_val: 0.9633 time: 0.6610s\n",
      "Epoch: 0056 loss_train: 1773.0773 acc_train: 0.9626 loss_val: 2201.5381 acc_val: 0.9636 time: 0.6148s\n",
      "Epoch: 0057 loss_train: 1772.7607 acc_train: 0.9627 loss_val: 2202.0542 acc_val: 0.9634 time: 0.5901s\n",
      "Epoch: 0058 loss_train: 1773.1783 acc_train: 0.9624 loss_val: 2202.6748 acc_val: 0.9629 time: 0.7460s\n",
      "Epoch: 0059 loss_train: 1772.7275 acc_train: 0.9628 loss_val: 2201.4448 acc_val: 0.9636 time: 0.7117s\n",
      "Epoch: 0060 loss_train: 1772.7227 acc_train: 0.9626 loss_val: 2202.4824 acc_val: 0.9632 time: 0.6652s\n",
      "Epoch: 0061 loss_train: 1772.4595 acc_train: 0.9629 loss_val: 2203.3503 acc_val: 0.9629 time: 0.6154s\n",
      "Epoch: 0062 loss_train: 1773.2036 acc_train: 0.9624 loss_val: 2201.5984 acc_val: 0.9635 time: 0.6730s\n",
      "Epoch: 0063 loss_train: 1773.1057 acc_train: 0.9624 loss_val: 2202.3862 acc_val: 0.9633 time: 0.6214s\n",
      "Epoch: 0064 loss_train: 1773.1663 acc_train: 0.9624 loss_val: 2201.8491 acc_val: 0.9634 time: 0.6170s\n",
      "Epoch: 0065 loss_train: 1772.3218 acc_train: 0.9629 loss_val: 2201.6816 acc_val: 0.9635 time: 0.5946s\n",
      "Epoch: 0066 loss_train: 1772.2544 acc_train: 0.9629 loss_val: 2201.3311 acc_val: 0.9636 time: 0.6667s\n",
      "Epoch: 0067 loss_train: 1773.5311 acc_train: 0.9622 loss_val: 2201.9973 acc_val: 0.9632 time: 0.6987s\n",
      "Epoch: 0068 loss_train: 1772.2151 acc_train: 0.9629 loss_val: 2201.6841 acc_val: 0.9633 time: 0.6508s\n",
      "Epoch: 0069 loss_train: 1772.5248 acc_train: 0.9627 loss_val: 2201.5737 acc_val: 0.9635 time: 0.6424s\n",
      "Epoch: 0070 loss_train: 1772.7689 acc_train: 0.9626 loss_val: 2201.9272 acc_val: 0.9634 time: 0.6366s\n",
      "Epoch: 0071 loss_train: 1771.9780 acc_train: 0.9631 loss_val: 2201.6001 acc_val: 0.9635 time: 0.6791s\n",
      "Epoch: 0072 loss_train: 1772.4862 acc_train: 0.9629 loss_val: 2202.2505 acc_val: 0.9633 time: 0.6905s\n",
      "Epoch: 0073 loss_train: 1772.7094 acc_train: 0.9627 loss_val: 2201.8247 acc_val: 0.9633 time: 0.6649s\n",
      "Epoch: 0074 loss_train: 1772.4044 acc_train: 0.9628 loss_val: 2201.6416 acc_val: 0.9635 time: 0.6128s\n",
      "Epoch: 0075 loss_train: 1772.3529 acc_train: 0.9627 loss_val: 2201.9084 acc_val: 0.9635 time: 0.6198s\n",
      "Epoch: 0076 loss_train: 1772.6653 acc_train: 0.9627 loss_val: 2202.0879 acc_val: 0.9633 time: 0.6827s\n",
      "Epoch: 0077 loss_train: 1772.6456 acc_train: 0.9627 loss_val: 2202.1479 acc_val: 0.9632 time: 0.6563s\n",
      "Epoch: 0078 loss_train: 1772.6250 acc_train: 0.9627 loss_val: 2202.2004 acc_val: 0.9632 time: 0.6730s\n",
      "Epoch: 0079 loss_train: 1772.3804 acc_train: 0.9628 loss_val: 2202.1555 acc_val: 0.9632 time: 0.6852s\n",
      "Epoch: 0080 loss_train: 1772.3319 acc_train: 0.9629 loss_val: 2201.9600 acc_val: 0.9632 time: 0.6365s\n",
      "Epoch: 0081 loss_train: 1772.3684 acc_train: 0.9628 loss_val: 2201.5867 acc_val: 0.9634 time: 0.5818s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0082 loss_train: 1772.9731 acc_train: 0.9626 loss_val: 2202.4473 acc_val: 0.9632 time: 0.8643s\n",
      "Epoch: 0083 loss_train: 1773.0850 acc_train: 0.9626 loss_val: 2202.0564 acc_val: 0.9633 time: 0.7481s\n",
      "Epoch: 0084 loss_train: 1772.9656 acc_train: 0.9627 loss_val: 2201.8384 acc_val: 0.9635 time: 0.6952s\n",
      "Epoch: 0085 loss_train: 1772.3230 acc_train: 0.9629 loss_val: 2201.5459 acc_val: 0.9636 time: 0.6659s\n",
      "Epoch: 0086 loss_train: 1772.5586 acc_train: 0.9628 loss_val: 2201.3513 acc_val: 0.9634 time: 0.5926s\n",
      "Epoch: 0087 loss_train: 1772.8250 acc_train: 0.9627 loss_val: 2202.0757 acc_val: 0.9633 time: 0.6953s\n",
      "Epoch: 0088 loss_train: 1772.3594 acc_train: 0.9628 loss_val: 2201.3726 acc_val: 0.9636 time: 0.6151s\n",
      "Epoch: 0089 loss_train: 1772.5969 acc_train: 0.9628 loss_val: 2201.7280 acc_val: 0.9633 time: 0.6230s\n",
      "Epoch: 0090 loss_train: 1772.6995 acc_train: 0.9628 loss_val: 2201.7283 acc_val: 0.9634 time: 0.6347s\n",
      "Epoch: 0091 loss_train: 1772.1119 acc_train: 0.9630 loss_val: 2201.9194 acc_val: 0.9636 time: 0.6724s\n",
      "Epoch: 0092 loss_train: 1772.0875 acc_train: 0.9631 loss_val: 2201.3779 acc_val: 0.9636 time: 0.6560s\n",
      "Epoch: 0093 loss_train: 1772.7223 acc_train: 0.9628 loss_val: 2201.8423 acc_val: 0.9634 time: 0.6205s\n",
      "Epoch: 0094 loss_train: 1772.2998 acc_train: 0.9629 loss_val: 2201.7939 acc_val: 0.9634 time: 0.5891s\n",
      "Epoch: 0095 loss_train: 1772.9196 acc_train: 0.9627 loss_val: 2201.5186 acc_val: 0.9636 time: 0.6592s\n",
      "Epoch: 0096 loss_train: 1772.5359 acc_train: 0.9629 loss_val: 2201.7017 acc_val: 0.9635 time: 0.7123s\n",
      "Epoch: 0097 loss_train: 1772.1631 acc_train: 0.9629 loss_val: 2201.9150 acc_val: 0.9634 time: 0.6110s\n",
      "Epoch: 0098 loss_train: 1772.1733 acc_train: 0.9629 loss_val: 2201.9780 acc_val: 0.9633 time: 0.6254s\n",
      "Epoch: 0099 loss_train: 1772.1799 acc_train: 0.9630 loss_val: 2202.4150 acc_val: 0.9631 time: 0.6051s\n",
      "Epoch: 0100 loss_train: 1772.4092 acc_train: 0.9629 loss_val: 2201.9824 acc_val: 0.9633 time: 0.6532s\n",
      "Epoch: 0101 loss_train: 1772.1399 acc_train: 0.9630 loss_val: 2201.8430 acc_val: 0.9633 time: 0.5945s\n",
      "Epoch: 0102 loss_train: 1772.3525 acc_train: 0.9628 loss_val: 2201.6318 acc_val: 0.9635 time: 0.6446s\n",
      "Epoch: 0103 loss_train: 1772.7266 acc_train: 0.9628 loss_val: 2202.1743 acc_val: 0.9633 time: 0.6031s\n",
      "Epoch: 0104 loss_train: 1772.6283 acc_train: 0.9628 loss_val: 2201.7554 acc_val: 0.9635 time: 0.6383s\n",
      "Epoch: 0105 loss_train: 1772.3899 acc_train: 0.9629 loss_val: 2201.7590 acc_val: 0.9634 time: 0.6178s\n",
      "Epoch: 0106 loss_train: 1772.2970 acc_train: 0.9630 loss_val: 2202.5640 acc_val: 0.9632 time: 0.6873s\n",
      "Epoch: 0107 loss_train: 1772.2283 acc_train: 0.9628 loss_val: 2201.4495 acc_val: 0.9636 time: 0.7445s\n",
      "Epoch: 0108 loss_train: 1772.6508 acc_train: 0.9628 loss_val: 2201.6665 acc_val: 0.9634 time: 0.7565s\n",
      "Epoch: 0109 loss_train: 1772.9675 acc_train: 0.9626 loss_val: 2201.3840 acc_val: 0.9635 time: 0.6049s\n",
      "Epoch: 0110 loss_train: 1772.0690 acc_train: 0.9630 loss_val: 2201.6572 acc_val: 0.9634 time: 0.5795s\n",
      "Epoch: 0111 loss_train: 1772.5889 acc_train: 0.9629 loss_val: 2201.3977 acc_val: 0.9636 time: 0.6713s\n",
      "Epoch: 0112 loss_train: 1773.0433 acc_train: 0.9626 loss_val: 2201.3281 acc_val: 0.9636 time: 0.5577s\n",
      "Epoch: 0113 loss_train: 1772.6033 acc_train: 0.9626 loss_val: 2201.3296 acc_val: 0.9636 time: 0.6602s\n",
      "Epoch: 0114 loss_train: 1772.5564 acc_train: 0.9629 loss_val: 2202.2959 acc_val: 0.9633 time: 0.6810s\n",
      "Epoch: 0115 loss_train: 1772.4335 acc_train: 0.9629 loss_val: 2201.4609 acc_val: 0.9636 time: 0.6436s\n",
      "Epoch: 0116 loss_train: 1772.3594 acc_train: 0.9629 loss_val: 2201.5151 acc_val: 0.9636 time: 0.6622s\n",
      "Epoch: 0117 loss_train: 1772.3059 acc_train: 0.9629 loss_val: 2201.6377 acc_val: 0.9636 time: 0.6771s\n",
      "Epoch: 0118 loss_train: 1772.4312 acc_train: 0.9629 loss_val: 2201.6226 acc_val: 0.9635 time: 0.6042s\n",
      "Epoch: 0119 loss_train: 1772.4949 acc_train: 0.9628 loss_val: 2202.0083 acc_val: 0.9633 time: 0.6408s\n",
      "Epoch: 0120 loss_train: 1772.1888 acc_train: 0.9629 loss_val: 2202.0933 acc_val: 0.9633 time: 0.6076s\n",
      "Epoch: 0121 loss_train: 1772.1509 acc_train: 0.9630 loss_val: 2201.4241 acc_val: 0.9636 time: 0.6282s\n",
      "Epoch: 0122 loss_train: 1772.4780 acc_train: 0.9628 loss_val: 2201.4241 acc_val: 0.9637 time: 0.6342s\n",
      "Epoch: 0123 loss_train: 1772.1202 acc_train: 0.9630 loss_val: 2201.7329 acc_val: 0.9635 time: 0.5919s\n",
      "Epoch: 0124 loss_train: 1772.4910 acc_train: 0.9628 loss_val: 2201.5254 acc_val: 0.9636 time: 0.6101s\n",
      "Epoch: 0125 loss_train: 1772.7623 acc_train: 0.9627 loss_val: 2201.5857 acc_val: 0.9635 time: 0.6209s\n",
      "Epoch: 0126 loss_train: 1772.6881 acc_train: 0.9628 loss_val: 2201.5562 acc_val: 0.9636 time: 0.6593s\n",
      "Epoch: 0127 loss_train: 1771.9580 acc_train: 0.9631 loss_val: 2201.5500 acc_val: 0.9635 time: 0.6555s\n",
      "Epoch: 0128 loss_train: 1772.3505 acc_train: 0.9629 loss_val: 2201.3049 acc_val: 0.9637 time: 0.6522s\n",
      "Epoch: 0129 loss_train: 1772.1519 acc_train: 0.9630 loss_val: 2201.3545 acc_val: 0.9636 time: 0.6406s\n",
      "Epoch: 0130 loss_train: 1772.7330 acc_train: 0.9628 loss_val: 2202.0015 acc_val: 0.9634 time: 0.6996s\n",
      "Epoch: 0131 loss_train: 1772.4199 acc_train: 0.9628 loss_val: 2201.3125 acc_val: 0.9636 time: 0.6934s\n",
      "Epoch: 0132 loss_train: 1772.2407 acc_train: 0.9630 loss_val: 2201.2935 acc_val: 0.9636 time: 0.6817s\n",
      "Epoch: 0133 loss_train: 1772.3311 acc_train: 0.9630 loss_val: 2201.4895 acc_val: 0.9635 time: 0.6064s\n",
      "Epoch: 0134 loss_train: 1772.6182 acc_train: 0.9628 loss_val: 2202.7529 acc_val: 0.9633 time: 0.6318s\n",
      "Epoch: 0135 loss_train: 1772.3708 acc_train: 0.9629 loss_val: 2201.3254 acc_val: 0.9637 time: 0.6262s\n",
      "Epoch: 0136 loss_train: 1771.9749 acc_train: 0.9630 loss_val: 2201.9194 acc_val: 0.9635 time: 0.5760s\n",
      "Epoch: 0137 loss_train: 1772.3174 acc_train: 0.9630 loss_val: 2201.4712 acc_val: 0.9636 time: 0.5870s\n",
      "Epoch: 0138 loss_train: 1772.4551 acc_train: 0.9629 loss_val: 2201.2959 acc_val: 0.9637 time: 0.6185s\n",
      "Epoch: 0139 loss_train: 1772.3956 acc_train: 0.9629 loss_val: 2202.0144 acc_val: 0.9635 time: 0.6194s\n",
      "Epoch: 0140 loss_train: 1772.1105 acc_train: 0.9630 loss_val: 2201.5754 acc_val: 0.9635 time: 0.5706s\n",
      "Epoch: 0141 loss_train: 1772.1819 acc_train: 0.9630 loss_val: 2201.0640 acc_val: 0.9637 time: 0.6027s\n",
      "Epoch: 0142 loss_train: 1772.2783 acc_train: 0.9631 loss_val: 2201.7261 acc_val: 0.9636 time: 0.6439s\n",
      "Epoch: 0143 loss_train: 1772.0134 acc_train: 0.9631 loss_val: 2201.7219 acc_val: 0.9634 time: 0.6178s\n",
      "Epoch: 0144 loss_train: 1772.1382 acc_train: 0.9630 loss_val: 2201.3967 acc_val: 0.9636 time: 0.5726s\n",
      "Epoch: 0145 loss_train: 1772.3569 acc_train: 0.9629 loss_val: 2201.4971 acc_val: 0.9635 time: 0.5930s\n",
      "Epoch: 0146 loss_train: 1772.3900 acc_train: 0.9629 loss_val: 2201.5725 acc_val: 0.9635 time: 0.5606s\n",
      "Epoch: 0147 loss_train: 1772.5031 acc_train: 0.9628 loss_val: 2201.6172 acc_val: 0.9635 time: 0.6825s\n",
      "Epoch: 0148 loss_train: 1772.2488 acc_train: 0.9630 loss_val: 2201.4556 acc_val: 0.9636 time: 0.5698s\n",
      "Epoch: 0149 loss_train: 1772.2859 acc_train: 0.9629 loss_val: 2201.8254 acc_val: 0.9635 time: 0.6139s\n",
      "Epoch: 0150 loss_train: 1772.2975 acc_train: 0.9629 loss_val: 2201.1138 acc_val: 0.9637 time: 0.6226s\n",
      "Epoch: 0151 loss_train: 1772.3828 acc_train: 0.9628 loss_val: 2201.3669 acc_val: 0.9636 time: 0.6446s\n",
      "Epoch: 0152 loss_train: 1772.6770 acc_train: 0.9628 loss_val: 2201.3828 acc_val: 0.9636 time: 0.5641s\n",
      "Epoch: 0153 loss_train: 1772.2283 acc_train: 0.9630 loss_val: 2201.5156 acc_val: 0.9636 time: 0.6227s\n",
      "Epoch: 0154 loss_train: 1771.8992 acc_train: 0.9631 loss_val: 2201.3643 acc_val: 0.9636 time: 0.5677s\n",
      "Epoch: 0155 loss_train: 1772.1729 acc_train: 0.9630 loss_val: 2201.7192 acc_val: 0.9635 time: 0.6127s\n",
      "Epoch: 0156 loss_train: 1772.3260 acc_train: 0.9628 loss_val: 2201.6399 acc_val: 0.9635 time: 0.5870s\n",
      "Epoch: 0157 loss_train: 1773.2278 acc_train: 0.9626 loss_val: 2201.3364 acc_val: 0.9636 time: 0.6815s\n",
      "Epoch: 0158 loss_train: 1772.1034 acc_train: 0.9630 loss_val: 2201.1672 acc_val: 0.9637 time: 0.6288s\n",
      "Epoch: 0159 loss_train: 1772.0775 acc_train: 0.9630 loss_val: 2202.0356 acc_val: 0.9633 time: 0.6735s\n",
      "Epoch: 0160 loss_train: 1772.2451 acc_train: 0.9630 loss_val: 2201.2671 acc_val: 0.9637 time: 0.5827s\n",
      "Epoch: 0161 loss_train: 1772.4067 acc_train: 0.9629 loss_val: 2201.0798 acc_val: 0.9637 time: 0.6799s\n",
      "Epoch: 0162 loss_train: 1772.3687 acc_train: 0.9630 loss_val: 2201.8071 acc_val: 0.9635 time: 0.5740s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0163 loss_train: 1772.3411 acc_train: 0.9629 loss_val: 2201.4639 acc_val: 0.9636 time: 0.6008s\n",
      "Epoch: 0164 loss_train: 1772.3208 acc_train: 0.9629 loss_val: 2201.4717 acc_val: 0.9635 time: 0.6415s\n",
      "Epoch: 0165 loss_train: 1772.7601 acc_train: 0.9627 loss_val: 2201.9971 acc_val: 0.9635 time: 0.6338s\n",
      "Epoch: 0166 loss_train: 1772.0277 acc_train: 0.9631 loss_val: 2202.0537 acc_val: 0.9635 time: 0.6128s\n",
      "Epoch: 0167 loss_train: 1771.8601 acc_train: 0.9631 loss_val: 2201.3157 acc_val: 0.9636 time: 0.6037s\n",
      "Epoch: 0168 loss_train: 1772.3959 acc_train: 0.9629 loss_val: 2202.1816 acc_val: 0.9634 time: 0.6121s\n",
      "Epoch: 0169 loss_train: 1771.9132 acc_train: 0.9631 loss_val: 2201.9905 acc_val: 0.9634 time: 0.6375s\n",
      "Epoch: 0170 loss_train: 1771.9006 acc_train: 0.9631 loss_val: 2201.3743 acc_val: 0.9636 time: 0.6151s\n",
      "Epoch: 0171 loss_train: 1772.2080 acc_train: 0.9630 loss_val: 2201.6567 acc_val: 0.9636 time: 0.6131s\n",
      "Epoch: 0172 loss_train: 1772.3909 acc_train: 0.9629 loss_val: 2201.5469 acc_val: 0.9635 time: 0.6602s\n",
      "Epoch: 0173 loss_train: 1771.9348 acc_train: 0.9631 loss_val: 2201.2915 acc_val: 0.9637 time: 0.6142s\n",
      "Epoch: 0174 loss_train: 1771.8240 acc_train: 0.9631 loss_val: 2201.0498 acc_val: 0.9637 time: 0.6322s\n",
      "Epoch: 0175 loss_train: 1771.9631 acc_train: 0.9631 loss_val: 2201.7524 acc_val: 0.9635 time: 0.5885s\n",
      "Epoch: 0176 loss_train: 1772.0426 acc_train: 0.9631 loss_val: 2200.9695 acc_val: 0.9637 time: 0.6442s\n",
      "Epoch: 0177 loss_train: 1772.3175 acc_train: 0.9629 loss_val: 2201.4038 acc_val: 0.9636 time: 0.6759s\n",
      "Epoch: 0178 loss_train: 1772.5303 acc_train: 0.9628 loss_val: 2201.6152 acc_val: 0.9636 time: 0.8496s\n",
      "Epoch: 0179 loss_train: 1772.4562 acc_train: 0.9629 loss_val: 2201.8936 acc_val: 0.9634 time: 0.8543s\n",
      "Epoch: 0180 loss_train: 1771.8650 acc_train: 0.9631 loss_val: 2201.9402 acc_val: 0.9634 time: 0.7578s\n",
      "Epoch: 0181 loss_train: 1772.3435 acc_train: 0.9629 loss_val: 2201.2283 acc_val: 0.9637 time: 0.8543s\n",
      "Epoch: 0182 loss_train: 1771.7849 acc_train: 0.9631 loss_val: 2201.3252 acc_val: 0.9637 time: 0.8850s\n",
      "Epoch: 0183 loss_train: 1771.9568 acc_train: 0.9631 loss_val: 2201.4985 acc_val: 0.9636 time: 0.8336s\n",
      "Epoch: 0184 loss_train: 1772.3522 acc_train: 0.9630 loss_val: 2201.5227 acc_val: 0.9635 time: 0.7561s\n",
      "Epoch: 0185 loss_train: 1771.9824 acc_train: 0.9631 loss_val: 2201.0215 acc_val: 0.9637 time: 0.7777s\n",
      "Epoch: 0186 loss_train: 1772.0543 acc_train: 0.9631 loss_val: 2201.4692 acc_val: 0.9636 time: 0.6973s\n",
      "Epoch: 0187 loss_train: 1772.3529 acc_train: 0.9630 loss_val: 2201.3184 acc_val: 0.9636 time: 0.7840s\n",
      "Epoch: 0188 loss_train: 1772.1260 acc_train: 0.9629 loss_val: 2201.4912 acc_val: 0.9635 time: 0.7168s\n",
      "Epoch: 0189 loss_train: 1772.3225 acc_train: 0.9629 loss_val: 2201.2651 acc_val: 0.9637 time: 0.7818s\n",
      "Epoch: 0190 loss_train: 1771.9796 acc_train: 0.9631 loss_val: 2202.0122 acc_val: 0.9634 time: 0.7687s\n",
      "Epoch: 0191 loss_train: 1771.9633 acc_train: 0.9631 loss_val: 2201.4021 acc_val: 0.9636 time: 0.6501s\n",
      "Epoch: 0192 loss_train: 1771.9227 acc_train: 0.9631 loss_val: 2201.0850 acc_val: 0.9637 time: 0.7298s\n",
      "Epoch: 0193 loss_train: 1773.3707 acc_train: 0.9627 loss_val: 2201.5166 acc_val: 0.9636 time: 0.7452s\n",
      "Epoch: 0194 loss_train: 1772.0640 acc_train: 0.9631 loss_val: 2201.4089 acc_val: 0.9636 time: 0.7506s\n",
      "Epoch: 0195 loss_train: 1772.1465 acc_train: 0.9630 loss_val: 2202.0073 acc_val: 0.9634 time: 0.7680s\n",
      "Epoch: 0196 loss_train: 1771.8513 acc_train: 0.9631 loss_val: 2201.1929 acc_val: 0.9637 time: 0.6565s\n",
      "Epoch: 0197 loss_train: 1772.3649 acc_train: 0.9628 loss_val: 2201.8516 acc_val: 0.9634 time: 0.7484s\n",
      "Epoch: 0198 loss_train: 1772.0074 acc_train: 0.9630 loss_val: 2201.3975 acc_val: 0.9636 time: 0.8331s\n",
      "Epoch: 0199 loss_train: 1771.9319 acc_train: 0.9631 loss_val: 2201.2690 acc_val: 0.9636 time: 0.7705s\n",
      "Epoch: 0200 loss_train: 1772.0608 acc_train: 0.9631 loss_val: 2201.0669 acc_val: 0.9637 time: 0.8336s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 132.1082s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = np.sum([(F.binary_cross_entropy_with_logits(output[idx_test][:,i], labels[idx_test][:,i])*weight(labels[idx_test][:,i])).mean() for i in range(39)])\n",
    "    acc_test = accuracy_sample_class(threshold(output.detach().numpy()[idx_test]), labels.detach().numpy()[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 2129.0698 accuracy= 0.9651\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Attempt : try nn.BCEwithlogits with weights (inverse of the classes' occurence in the labeling vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Optimizer \n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.shape[1],\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr = args.lr, weight_decay = args.weight_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
