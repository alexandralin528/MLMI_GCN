{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the BlogCatalog Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our part, we chose to work with the BlogCatalog dataset. It is a graph dateset that represents a network of social relationships, where the nodes represent blogger authors and the labels reprensent the bloggers' interest such as *Education*, *Food* and *Health*. The problem we are trying to solve is a multi-label classification of nodes which means that a node (blogger) might have one corresponding label (interest). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into our ML-GCN method for multi label classification on graphs, we'll first take a look at our dataset, to get a better understanding of it and make the implementation easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the paper, the graph has over 10312 nodes with over 333983 edges connecting them. For each node, there 39 possible labels with iver 615 co-occurence relationships between them. \n",
    "The blog dataset is given in the form of two main csv files. The first one is called group-edges.csv. It gives the nodes and their corresponding labels. It should be noted that each node might have more than one label. The first step before moving along with classification would be to attribute each nodes its corresponding labels in the form of 39 lenghted vector, with the value 1 in the index corresponding to a related label and 0 otherwise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second file is called edges.csv and as its name indicates it represents edge relationships between different nodes to form the initial graph. This graph will be used to build the node graph along with extracting the adjacency matrix and the features vector of each one of the nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One first important step in any deep learning method is to load the dataset to make it usable for our deep/machine learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing that, it's important to know what we need for our algorithm (a simple GCN at first). As an input for our model, we will need the adjacency matrix of the graph (normalized with added self loops) $\\hat{A}$ and the features matrix $X$. To compute the train/validation loss, we will need the ground truth labels for the training/validation dataset. Last but not least, to feed our dataset to our GCN model, we need our set of nodes split somehow to three subsets for training, validation and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name): \n",
    "    print(\"Loading {} dataset...\".format(data_name))\n",
    "    edges_file = data_name + \"/edges.csv\"\n",
    "    node_label_file = data_name + \"/group-edges.csv\"\n",
    "    \n",
    "    # We'll first dive into the group_edges.csv file in order to extract a list of the nodes along with their \n",
    "    # corresponding labels\n",
    "    label_raw, nodes = [], []\n",
    "    with open(node_label_file) as file_to_read: \n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break \n",
    "            node, label = lines.split(\",\")\n",
    "            label_raw.append(int(label))\n",
    "            nodes.append(int(node))\n",
    "    unique_nodes = np.unique(nodes)\n",
    "    # Now we have a list of nodes and a list of their labels\n",
    "    # Since a node can have multiple labels, we should give each node a corresponding 39 lengthed vector that \n",
    "    # encodes 1 when the node has the label corresponding to the index and 0 otherwise.  \n",
    "    label_raw = np.array(label_raw)\n",
    "    nodes = np.array(nodes)\n",
    "    labels = np.zeros((unique_nodes.shape[0], 39))\n",
    "    for l in range(1, 40, 1):\n",
    "        indices = np.argwhere(label_raw == l).reshape(-1)\n",
    "        n_l = nodes[indices]\n",
    "        for n in n_l:\n",
    "            labels[n-1][l-1] = 1\n",
    "    \n",
    "    # Now we can build our BlogCatalog graph using the file edges.csv \n",
    "    \n",
    "    file_to_read = open(edges_file, 'rb')\n",
    "    print(file_to_read)\n",
    "    G = nx.read_edgelist(file_to_read, delimiter = \",\", nodetype = int)\n",
    "    \n",
    "    # Let's now extract our adjacency matrix from the graph \n",
    "    A = nx.adjacency_matrix(G, nodelist = unique_nodes) # Already a symmetric matrix \n",
    "    A = sp.coo_matrix(A.todense())\n",
    "    \n",
    "    # Let's extract the feature matrix as well\n",
    "    X = sp.csr_matrix(A)\n",
    "    \n",
    "    # As we saw in the paper, we need the normalized version of the adjacency matrix with the added self loops\n",
    "    A = normalize(A + sp.eye(A.shape[0]))\n",
    "    # X = normalize(X) --> Why do we need to do that ? \n",
    "    \n",
    "    # Let's define the train, validation and test sets \n",
    "    indices = np.arange(A.shape[0]).astype('int32')\n",
    "    # np.random.shuffle(indices)\n",
    "    idx_train = indices[:A.shape[0] // 3]\n",
    "    idx_val = indices[A.shape[0] // 3: (2 * A.shape[0]) // 3]\n",
    "    idx_test = indices[(2 * A.shape[0]) // 3:]\n",
    "    \n",
    "    # Convert to tensors \n",
    "    X = torch.FloatTensor(np.array(X.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "    A = sparse_mx_to_torch_sparse_tensor(A)\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    return A, X, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BlogCatalog dataset...\n",
      "<_io.BufferedReader name='BlogCatalog/edges.csv'>\n"
     ]
    }
   ],
   "source": [
    "A, X, labels, idx_train, idx_val, idx_test = load_data(\"BlogCatalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function that gives the accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple accuracy is equal to the number of correct predictions over the number of samples, in the multi-label classification, the accuracy can be defined in 2 ways: \n",
    "\n",
    "- Sample view : A sample is correctly classified when all of its labels are correctly classified and accuracy means the number of correctly classified samples over the total number of samples.\n",
    "- Sample-class view : Assume that the output of the classifier is a matrix with dimension N by C where N is the number of samples and C is the number of classes. Accuracy means how many of the N by C elements in this output are correctly classified elements divided by the number of elements of the matrix. \n",
    "\n",
    "We can implement both views in the following way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sample(output, labels):\n",
    "    \"\"\" \n",
    "    output and labels are tensors\n",
    "    output is of shape (N,C)\n",
    "    Labels is of shape (N,C)\n",
    "    Result : acc gives the accuracy computed according to the sample view\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    corr = np.sum(np.all(np.equal(output, labels), axis=1))\n",
    "    # corr is the number of equal rows and thus the number of correctly classified samples\n",
    "    acc = corr / N\n",
    "    return acc       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sample_class(output, labels):\n",
    "    \"\"\" \n",
    "    output is of shape (N,C)\n",
    "    Labels is of shape (N,C)\n",
    "    Result : acc gives the accuracy computed according to the sample-class view\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    C = labels.shape[1]\n",
    "    corr = np.sum(np.equal(output, labels))\n",
    "    # corr is the number of equal elements between labels and output and thus the number of correctly classified \n",
    "    # labels for each sample \n",
    "    acc = corr/(N*C)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(output):\n",
    "    output[output > 0.5] = 1\n",
    "    output[output <= 0.5] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the loss computed for our case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use a simple classifier over a multi-label and multi-class dataset, your loss function is the sum/average over the binary classifier of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, in our case, we are trying to apply a simple classifier to solve a multi label classification problem. There are a few things that we should pay attention to in order to succeed in buiding our model for this specific task. In summary:\n",
    "\n",
    "- Number of nodes in the output layer matches the number of labels. \n",
    "- A sigmoid function should be applied for each node in the output layer. \n",
    "- A binary cross-entropy loss is the loss that should be used over each one of the labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training phase : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#from pygcn.utils import load_data, accuracy_sample, accuracy_sample_class\n",
    "from pygcn.models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BlogCatalog dataset...\n",
      "<_io.BufferedReader name='BlogCatalog/edges.csv'>\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(\"BlogCatalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Optimizer \n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.shape[1],\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gc1): GraphConvolution (10312 -> 16)\n",
       "  (gc2): GraphConvolution (16 -> 39)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HSAIN KENZA\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "output = model(features, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = np.sum([F.binary_cross_entropy_with_logits(output[idx_train][:,i], labels[idx_train][:,i]) for i in range(39)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.9938, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_sample_class(threshold(output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5373797960356005"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = np.sum([F.binary_cross_entropy_with_logits(output[idx_val][:,i], labels[idx_val][:,i]) for i in range(39)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val = accuracy_sample_class(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5412367673060138"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = np.sum([F.binary_cross_entropy_with_logits(output[idx_val][:,i], labels[idx_val][:,i]) for i in range(39)])\n",
    "acc_val = accuracy_sample_class(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5690039763359519"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold(output.detach().numpy()[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    weight = torch.tensor([2.0,1.0])\n",
    "    loss_train = np.sum([(F.binary_cross_entropy_with_logits(output[idx_train][:,i], labels[idx_train][:,i]*weight).mean() for i in range(39)])\n",
    "    acc_train = accuracy_sample_class(threshold(output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    print(np.sum(threshold(output.detach().numpy()[idx_train])>0))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        #model.eval()\n",
    "        #output = model(features, adj)\n",
    "\n",
    "    loss_val = np.sum([(F.binary_cross_entropy_with_logits(output[idx_val][:,i], labels[idx_val][:,i])*weight).mean() for i in range(39)])\n",
    "    acc_val = accuracy_sample_class(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58335\n",
      "Epoch: 0001 loss_train: 36.8453 acc_train: 0.5547 loss_val: 36.8165 acc_val: 0.5572 time: 0.3541s\n",
      "56752\n",
      "Epoch: 0002 loss_train: 36.6824 acc_train: 0.5661 loss_val: 36.6596 acc_val: 0.5673 time: 0.3122s\n",
      "54927\n",
      "Epoch: 0003 loss_train: 36.4693 acc_train: 0.5786 loss_val: 36.4404 acc_val: 0.5796 time: 0.3271s\n",
      "53356\n",
      "Epoch: 0004 loss_train: 36.3028 acc_train: 0.5900 loss_val: 36.2703 acc_val: 0.5884 time: 0.4239s\n",
      "50579\n",
      "Epoch: 0005 loss_train: 36.0459 acc_train: 0.6094 loss_val: 36.0009 acc_val: 0.6096 time: 0.4338s\n",
      "44502\n",
      "Epoch: 0006 loss_train: 35.8528 acc_train: 0.6525 loss_val: 35.7934 acc_val: 0.6534 time: 0.3830s\n",
      "39081\n",
      "Epoch: 0007 loss_train: 35.6202 acc_train: 0.6914 loss_val: 35.5878 acc_val: 0.6929 time: 0.3271s\n",
      "36106\n",
      "Epoch: 0008 loss_train: 35.3937 acc_train: 0.7131 loss_val: 35.3487 acc_val: 0.7121 time: 0.3092s\n",
      "33550\n",
      "Epoch: 0009 loss_train: 35.1208 acc_train: 0.7310 loss_val: 35.0455 acc_val: 0.7256 time: 0.3293s\n",
      "31709\n",
      "Epoch: 0010 loss_train: 34.8532 acc_train: 0.7433 loss_val: 34.8378 acc_val: 0.7424 time: 0.3181s\n",
      "27313\n",
      "Epoch: 0011 loss_train: 34.6023 acc_train: 0.7731 loss_val: 34.5358 acc_val: 0.7713 time: 0.3710s\n",
      "23307\n",
      "Epoch: 0012 loss_train: 34.2750 acc_train: 0.8004 loss_val: 34.2214 acc_val: 0.7983 time: 0.3734s\n",
      "19022\n",
      "Epoch: 0013 loss_train: 34.0272 acc_train: 0.8301 loss_val: 34.0364 acc_val: 0.8303 time: 0.3451s\n",
      "16849\n",
      "Epoch: 0014 loss_train: 33.7122 acc_train: 0.8445 loss_val: 33.6345 acc_val: 0.8447 time: 0.3790s\n",
      "14639\n",
      "Epoch: 0015 loss_train: 33.4099 acc_train: 0.8602 loss_val: 33.3462 acc_val: 0.8601 time: 0.3371s\n",
      "13221\n",
      "Epoch: 0016 loss_train: 33.1996 acc_train: 0.8698 loss_val: 33.0827 acc_val: 0.8723 time: 0.3411s\n",
      "11067\n",
      "Epoch: 0017 loss_train: 32.7921 acc_train: 0.8850 loss_val: 32.7265 acc_val: 0.8851 time: 0.3541s\n",
      "9741\n",
      "Epoch: 0018 loss_train: 32.3668 acc_train: 0.8935 loss_val: 32.4131 acc_val: 0.8967 time: 0.3652s\n",
      "8501\n",
      "Epoch: 0019 loss_train: 32.2551 acc_train: 0.9023 loss_val: 32.1731 acc_val: 0.9044 time: 0.3820s\n",
      "7362\n",
      "Epoch: 0020 loss_train: 31.8411 acc_train: 0.9103 loss_val: 31.8457 acc_val: 0.9125 time: 0.3860s\n",
      "5857\n",
      "Epoch: 0021 loss_train: 31.4425 acc_train: 0.9209 loss_val: 31.4463 acc_val: 0.9213 time: 0.3301s\n",
      "2602\n",
      "Epoch: 0022 loss_train: 31.1657 acc_train: 0.9442 loss_val: 31.1584 acc_val: 0.9433 time: 0.3433s\n",
      "1158\n",
      "Epoch: 0023 loss_train: 30.9846 acc_train: 0.9548 loss_val: 30.8688 acc_val: 0.9548 time: 0.3572s\n",
      "613\n",
      "Epoch: 0024 loss_train: 30.6153 acc_train: 0.9587 loss_val: 30.5325 acc_val: 0.9588 time: 0.3391s\n",
      "456\n",
      "Epoch: 0025 loss_train: 30.3851 acc_train: 0.9600 loss_val: 30.3501 acc_val: 0.9609 time: 0.3281s\n",
      "240\n",
      "Epoch: 0026 loss_train: 30.1319 acc_train: 0.9614 loss_val: 30.0502 acc_val: 0.9617 time: 0.3341s\n",
      "209\n",
      "Epoch: 0027 loss_train: 29.8917 acc_train: 0.9617 loss_val: 29.9486 acc_val: 0.9619 time: 0.3261s\n",
      "150\n",
      "Epoch: 0028 loss_train: 29.5856 acc_train: 0.9620 loss_val: 29.4712 acc_val: 0.9629 time: 0.3690s\n",
      "107\n",
      "Epoch: 0029 loss_train: 29.5520 acc_train: 0.9624 loss_val: 29.5100 acc_val: 0.9624 time: 0.3979s\n",
      "116\n",
      "Epoch: 0030 loss_train: 29.3476 acc_train: 0.9623 loss_val: 29.3772 acc_val: 0.9624 time: 0.3441s\n",
      "101\n",
      "Epoch: 0031 loss_train: 29.2775 acc_train: 0.9624 loss_val: 29.1297 acc_val: 0.9628 time: 0.3460s\n",
      "113\n",
      "Epoch: 0032 loss_train: 28.9515 acc_train: 0.9623 loss_val: 28.9653 acc_val: 0.9631 time: 0.3461s\n",
      "95\n",
      "Epoch: 0033 loss_train: 28.9046 acc_train: 0.9625 loss_val: 28.8405 acc_val: 0.9633 time: 0.3470s\n",
      "109\n",
      "Epoch: 0034 loss_train: 28.7708 acc_train: 0.9624 loss_val: 28.6830 acc_val: 0.9633 time: 0.3620s\n",
      "63\n",
      "Epoch: 0035 loss_train: 28.5978 acc_train: 0.9627 loss_val: 28.6213 acc_val: 0.9629 time: 0.3610s\n",
      "126\n",
      "Epoch: 0036 loss_train: 28.6915 acc_train: 0.9623 loss_val: 28.5890 acc_val: 0.9633 time: 0.3570s\n",
      "114\n",
      "Epoch: 0037 loss_train: 28.4825 acc_train: 0.9623 loss_val: 28.4679 acc_val: 0.9627 time: 0.4159s\n",
      "64\n",
      "Epoch: 0038 loss_train: 28.4236 acc_train: 0.9627 loss_val: 28.3975 acc_val: 0.9631 time: 0.3745s\n",
      "67\n",
      "Epoch: 0039 loss_train: 28.3049 acc_train: 0.9627 loss_val: 28.3084 acc_val: 0.9633 time: 0.3311s\n",
      "32\n",
      "Epoch: 0040 loss_train: 28.2692 acc_train: 0.9629 loss_val: 28.2064 acc_val: 0.9636 time: 0.3780s\n",
      "65\n",
      "Epoch: 0041 loss_train: 28.1855 acc_train: 0.9627 loss_val: 28.1617 acc_val: 0.9632 time: 0.3643s\n",
      "56\n",
      "Epoch: 0042 loss_train: 28.1810 acc_train: 0.9627 loss_val: 28.1191 acc_val: 0.9632 time: 0.3690s\n",
      "60\n",
      "Epoch: 0043 loss_train: 28.1276 acc_train: 0.9628 loss_val: 28.1021 acc_val: 0.9632 time: 0.4119s\n",
      "70\n",
      "Epoch: 0044 loss_train: 28.0262 acc_train: 0.9627 loss_val: 27.9862 acc_val: 0.9633 time: 0.3883s\n",
      "41\n",
      "Epoch: 0045 loss_train: 28.0401 acc_train: 0.9629 loss_val: 28.0083 acc_val: 0.9633 time: 0.3450s\n",
      "25\n",
      "Epoch: 0046 loss_train: 27.8362 acc_train: 0.9630 loss_val: 27.8311 acc_val: 0.9635 time: 0.3491s\n",
      "37\n",
      "Epoch: 0047 loss_train: 27.9086 acc_train: 0.9629 loss_val: 27.8682 acc_val: 0.9634 time: 0.3531s\n",
      "39\n",
      "Epoch: 0048 loss_train: 27.8839 acc_train: 0.9629 loss_val: 27.8742 acc_val: 0.9634 time: 0.3979s\n",
      "14\n",
      "Epoch: 0049 loss_train: 27.8484 acc_train: 0.9631 loss_val: 27.8207 acc_val: 0.9634 time: 0.4189s\n",
      "26\n",
      "Epoch: 0050 loss_train: 27.7973 acc_train: 0.9630 loss_val: 27.7144 acc_val: 0.9636 time: 0.3970s\n",
      "27\n",
      "Epoch: 0051 loss_train: 27.7690 acc_train: 0.9629 loss_val: 27.7138 acc_val: 0.9634 time: 0.3660s\n",
      "38\n",
      "Epoch: 0052 loss_train: 27.7979 acc_train: 0.9629 loss_val: 27.7284 acc_val: 0.9635 time: 0.3850s\n",
      "53\n",
      "Epoch: 0053 loss_train: 27.7007 acc_train: 0.9628 loss_val: 27.6976 acc_val: 0.9634 time: 0.3890s\n",
      "30\n",
      "Epoch: 0054 loss_train: 27.7088 acc_train: 0.9629 loss_val: 27.6989 acc_val: 0.9634 time: 0.3962s\n",
      "17\n",
      "Epoch: 0055 loss_train: 27.6133 acc_train: 0.9630 loss_val: 27.6083 acc_val: 0.9634 time: 0.3640s\n",
      "34\n",
      "Epoch: 0056 loss_train: 27.6032 acc_train: 0.9629 loss_val: 27.5848 acc_val: 0.9635 time: 0.3690s\n",
      "33\n",
      "Epoch: 0057 loss_train: 27.6422 acc_train: 0.9630 loss_val: 27.6416 acc_val: 0.9635 time: 0.3833s\n",
      "24\n",
      "Epoch: 0058 loss_train: 27.5431 acc_train: 0.9630 loss_val: 27.5141 acc_val: 0.9636 time: 0.4155s\n",
      "44\n",
      "Epoch: 0059 loss_train: 27.6240 acc_train: 0.9629 loss_val: 27.6013 acc_val: 0.9632 time: 0.3927s\n",
      "17\n",
      "Epoch: 0060 loss_train: 27.6382 acc_train: 0.9630 loss_val: 27.5705 acc_val: 0.9635 time: 0.3851s\n",
      "12\n",
      "Epoch: 0061 loss_train: 27.5619 acc_train: 0.9631 loss_val: 27.5771 acc_val: 0.9635 time: 0.3875s\n",
      "31\n",
      "Epoch: 0062 loss_train: 27.5279 acc_train: 0.9630 loss_val: 27.5010 acc_val: 0.9635 time: 0.3727s\n",
      "42\n",
      "Epoch: 0063 loss_train: 27.5606 acc_train: 0.9628 loss_val: 27.5348 acc_val: 0.9635 time: 0.4000s\n",
      "20\n",
      "Epoch: 0064 loss_train: 27.5055 acc_train: 0.9630 loss_val: 27.5482 acc_val: 0.9635 time: 0.3879s\n",
      "24\n",
      "Epoch: 0065 loss_train: 27.5641 acc_train: 0.9630 loss_val: 27.5408 acc_val: 0.9635 time: 0.3822s\n",
      "25\n",
      "Epoch: 0066 loss_train: 27.4771 acc_train: 0.9630 loss_val: 27.4683 acc_val: 0.9636 time: 0.3939s\n",
      "34\n",
      "Epoch: 0067 loss_train: 27.5026 acc_train: 0.9629 loss_val: 27.5149 acc_val: 0.9634 time: 0.4069s\n",
      "51\n",
      "Epoch: 0068 loss_train: 27.4782 acc_train: 0.9628 loss_val: 27.4508 acc_val: 0.9635 time: 0.3830s\n",
      "33\n",
      "Epoch: 0069 loss_train: 27.4787 acc_train: 0.9629 loss_val: 27.4452 acc_val: 0.9635 time: 0.3730s\n",
      "42\n",
      "Epoch: 0070 loss_train: 27.4282 acc_train: 0.9629 loss_val: 27.3851 acc_val: 0.9637 time: 0.4039s\n",
      "50\n",
      "Epoch: 0071 loss_train: 27.4494 acc_train: 0.9628 loss_val: 27.3927 acc_val: 0.9636 time: 0.3712s\n",
      "30\n",
      "Epoch: 0072 loss_train: 27.4217 acc_train: 0.9629 loss_val: 27.3988 acc_val: 0.9636 time: 0.4060s\n",
      "24\n",
      "Epoch: 0073 loss_train: 27.4683 acc_train: 0.9630 loss_val: 27.4062 acc_val: 0.9637 time: 0.4222s\n",
      "27\n",
      "Epoch: 0074 loss_train: 27.4238 acc_train: 0.9630 loss_val: 27.3945 acc_val: 0.9636 time: 0.3984s\n",
      "71\n",
      "Epoch: 0075 loss_train: 27.4266 acc_train: 0.9626 loss_val: 27.3977 acc_val: 0.9637 time: 0.4109s\n",
      "48\n",
      "Epoch: 0076 loss_train: 27.4239 acc_train: 0.9628 loss_val: 27.4048 acc_val: 0.9632 time: 0.3984s\n",
      "16\n",
      "Epoch: 0077 loss_train: 27.3430 acc_train: 0.9630 loss_val: 27.3364 acc_val: 0.9635 time: 0.4070s\n",
      "9\n",
      "Epoch: 0078 loss_train: 27.3570 acc_train: 0.9631 loss_val: 27.3757 acc_val: 0.9636 time: 0.3829s\n",
      "28\n",
      "Epoch: 0079 loss_train: 27.4177 acc_train: 0.9630 loss_val: 27.3848 acc_val: 0.9636 time: 0.3613s\n",
      "13\n",
      "Epoch: 0080 loss_train: 27.3488 acc_train: 0.9630 loss_val: 27.3330 acc_val: 0.9635 time: 0.4099s\n",
      "35\n",
      "Epoch: 0081 loss_train: 27.3590 acc_train: 0.9629 loss_val: 27.3664 acc_val: 0.9634 time: 0.3920s\n",
      "30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0082 loss_train: 27.3460 acc_train: 0.9629 loss_val: 27.3163 acc_val: 0.9637 time: 0.3820s\n",
      "12\n",
      "Epoch: 0083 loss_train: 27.3335 acc_train: 0.9631 loss_val: 27.3385 acc_val: 0.9635 time: 0.4059s\n",
      "36\n",
      "Epoch: 0084 loss_train: 27.3367 acc_train: 0.9629 loss_val: 27.3591 acc_val: 0.9634 time: 0.3920s\n",
      "21\n",
      "Epoch: 0085 loss_train: 27.3515 acc_train: 0.9630 loss_val: 27.3487 acc_val: 0.9637 time: 0.3780s\n",
      "26\n",
      "Epoch: 0086 loss_train: 27.3716 acc_train: 0.9630 loss_val: 27.3242 acc_val: 0.9635 time: 0.3790s\n",
      "26\n",
      "Epoch: 0087 loss_train: 27.3492 acc_train: 0.9630 loss_val: 27.3288 acc_val: 0.9636 time: 0.3710s\n",
      "16\n",
      "Epoch: 0088 loss_train: 27.3226 acc_train: 0.9631 loss_val: 27.3313 acc_val: 0.9635 time: 0.4338s\n",
      "15\n",
      "Epoch: 0089 loss_train: 27.3120 acc_train: 0.9630 loss_val: 27.3111 acc_val: 0.9636 time: 0.4628s\n",
      "27\n",
      "Epoch: 0090 loss_train: 27.3138 acc_train: 0.9630 loss_val: 27.2898 acc_val: 0.9635 time: 0.4819s\n",
      "38\n",
      "Epoch: 0091 loss_train: 27.3427 acc_train: 0.9629 loss_val: 27.3382 acc_val: 0.9635 time: 0.4448s\n",
      "22\n",
      "Epoch: 0092 loss_train: 27.3149 acc_train: 0.9630 loss_val: 27.3259 acc_val: 0.9635 time: 0.4009s\n",
      "41\n",
      "Epoch: 0093 loss_train: 27.3332 acc_train: 0.9629 loss_val: 27.3273 acc_val: 0.9636 time: 0.3772s\n",
      "6\n",
      "Epoch: 0094 loss_train: 27.2844 acc_train: 0.9631 loss_val: 27.2747 acc_val: 0.9635 time: 0.3969s\n",
      "24\n",
      "Epoch: 0095 loss_train: 27.2659 acc_train: 0.9630 loss_val: 27.2888 acc_val: 0.9637 time: 0.3949s\n",
      "15\n",
      "Epoch: 0096 loss_train: 27.3081 acc_train: 0.9630 loss_val: 27.3054 acc_val: 0.9634 time: 0.3720s\n",
      "16\n",
      "Epoch: 0097 loss_train: 27.2890 acc_train: 0.9630 loss_val: 27.2646 acc_val: 0.9637 time: 0.3911s\n",
      "6\n",
      "Epoch: 0098 loss_train: 27.2974 acc_train: 0.9631 loss_val: 27.2662 acc_val: 0.9636 time: 0.4239s\n",
      "6\n",
      "Epoch: 0099 loss_train: 27.2755 acc_train: 0.9631 loss_val: 27.2938 acc_val: 0.9635 time: 0.4079s\n",
      "19\n",
      "Epoch: 0100 loss_train: 27.2679 acc_train: 0.9630 loss_val: 27.2617 acc_val: 0.9636 time: 0.5047s\n",
      "11\n",
      "Epoch: 0101 loss_train: 27.2877 acc_train: 0.9631 loss_val: 27.2830 acc_val: 0.9636 time: 0.4281s\n",
      "36\n",
      "Epoch: 0102 loss_train: 27.2723 acc_train: 0.9629 loss_val: 27.2759 acc_val: 0.9636 time: 0.4787s\n",
      "21\n",
      "Epoch: 0103 loss_train: 27.2308 acc_train: 0.9630 loss_val: 27.2564 acc_val: 0.9637 time: 0.4568s\n",
      "5\n",
      "Epoch: 0104 loss_train: 27.2267 acc_train: 0.9631 loss_val: 27.2304 acc_val: 0.9637 time: 0.4209s\n",
      "7\n",
      "Epoch: 0105 loss_train: 27.2666 acc_train: 0.9631 loss_val: 27.2434 acc_val: 0.9636 time: 0.4099s\n",
      "15\n",
      "Epoch: 0106 loss_train: 27.2740 acc_train: 0.9630 loss_val: 27.2678 acc_val: 0.9637 time: 0.3900s\n",
      "10\n",
      "Epoch: 0107 loss_train: 27.2605 acc_train: 0.9631 loss_val: 27.2653 acc_val: 0.9636 time: 0.4009s\n",
      "19\n",
      "Epoch: 0108 loss_train: 27.2455 acc_train: 0.9630 loss_val: 27.2168 acc_val: 0.9637 time: 0.3980s\n",
      "20\n",
      "Epoch: 0109 loss_train: 27.2375 acc_train: 0.9630 loss_val: 27.2341 acc_val: 0.9637 time: 0.4089s\n",
      "25\n",
      "Epoch: 0110 loss_train: 27.2419 acc_train: 0.9630 loss_val: 27.2563 acc_val: 0.9636 time: 0.3939s\n",
      "1\n",
      "Epoch: 0111 loss_train: 27.2213 acc_train: 0.9631 loss_val: 27.2306 acc_val: 0.9637 time: 0.4161s\n",
      "4\n",
      "Epoch: 0112 loss_train: 27.2337 acc_train: 0.9631 loss_val: 27.2415 acc_val: 0.9636 time: 0.3800s\n",
      "2\n",
      "Epoch: 0113 loss_train: 27.2235 acc_train: 0.9631 loss_val: 27.2320 acc_val: 0.9635 time: 0.3929s\n",
      "34\n",
      "Epoch: 0114 loss_train: 27.2848 acc_train: 0.9629 loss_val: 27.2407 acc_val: 0.9635 time: 0.4050s\n",
      "34\n",
      "Epoch: 0115 loss_train: 27.2630 acc_train: 0.9630 loss_val: 27.2515 acc_val: 0.9636 time: 0.3959s\n",
      "26\n",
      "Epoch: 0116 loss_train: 27.2494 acc_train: 0.9630 loss_val: 27.2317 acc_val: 0.9636 time: 0.3860s\n",
      "17\n",
      "Epoch: 0117 loss_train: 27.2887 acc_train: 0.9630 loss_val: 27.2408 acc_val: 0.9636 time: 0.4141s\n",
      "30\n",
      "Epoch: 0118 loss_train: 27.2350 acc_train: 0.9629 loss_val: 27.2280 acc_val: 0.9637 time: 0.4109s\n",
      "15\n",
      "Epoch: 0119 loss_train: 27.2690 acc_train: 0.9631 loss_val: 27.2309 acc_val: 0.9636 time: 0.3999s\n",
      "10\n",
      "Epoch: 0120 loss_train: 27.2214 acc_train: 0.9631 loss_val: 27.2224 acc_val: 0.9636 time: 0.4239s\n",
      "21\n",
      "Epoch: 0121 loss_train: 27.2088 acc_train: 0.9630 loss_val: 27.2082 acc_val: 0.9637 time: 0.3870s\n",
      "5\n",
      "Epoch: 0122 loss_train: 27.1827 acc_train: 0.9631 loss_val: 27.2055 acc_val: 0.9637 time: 0.3960s\n",
      "59\n",
      "Epoch: 0123 loss_train: 27.2110 acc_train: 0.9627 loss_val: 27.2009 acc_val: 0.9637 time: 0.4229s\n",
      "38\n",
      "Epoch: 0124 loss_train: 27.2167 acc_train: 0.9629 loss_val: 27.2281 acc_val: 0.9636 time: 0.3851s\n",
      "15\n",
      "Epoch: 0125 loss_train: 27.2318 acc_train: 0.9630 loss_val: 27.2333 acc_val: 0.9636 time: 0.4089s\n",
      "8\n",
      "Epoch: 0126 loss_train: 27.2255 acc_train: 0.9631 loss_val: 27.2063 acc_val: 0.9637 time: 0.4522s\n",
      "45\n",
      "Epoch: 0127 loss_train: 27.2074 acc_train: 0.9628 loss_val: 27.2000 acc_val: 0.9636 time: 0.5136s\n",
      "33\n",
      "Epoch: 0128 loss_train: 27.2268 acc_train: 0.9629 loss_val: 27.2119 acc_val: 0.9636 time: 0.4879s\n",
      "5\n",
      "Epoch: 0129 loss_train: 27.1905 acc_train: 0.9631 loss_val: 27.2147 acc_val: 0.9636 time: 0.4209s\n",
      "21\n",
      "Epoch: 0130 loss_train: 27.1982 acc_train: 0.9630 loss_val: 27.1895 acc_val: 0.9637 time: 0.4228s\n",
      "11\n",
      "Epoch: 0131 loss_train: 27.1842 acc_train: 0.9631 loss_val: 27.1836 acc_val: 0.9637 time: 0.4279s\n",
      "17\n",
      "Epoch: 0132 loss_train: 27.2250 acc_train: 0.9630 loss_val: 27.2117 acc_val: 0.9636 time: 0.3995s\n",
      "17\n",
      "Epoch: 0133 loss_train: 27.1907 acc_train: 0.9630 loss_val: 27.1951 acc_val: 0.9637 time: 0.3850s\n",
      "29\n",
      "Epoch: 0134 loss_train: 27.1930 acc_train: 0.9629 loss_val: 27.1933 acc_val: 0.9636 time: 0.4209s\n",
      "34\n",
      "Epoch: 0135 loss_train: 27.2195 acc_train: 0.9629 loss_val: 27.2008 acc_val: 0.9637 time: 0.3949s\n",
      "16\n",
      "Epoch: 0136 loss_train: 27.1709 acc_train: 0.9630 loss_val: 27.1775 acc_val: 0.9637 time: 0.3999s\n",
      "2\n",
      "Epoch: 0137 loss_train: 27.1718 acc_train: 0.9631 loss_val: 27.1863 acc_val: 0.9637 time: 0.3799s\n",
      "2\n",
      "Epoch: 0138 loss_train: 27.1603 acc_train: 0.9631 loss_val: 27.1567 acc_val: 0.9637 time: 0.4020s\n",
      "11\n",
      "Epoch: 0139 loss_train: 27.2119 acc_train: 0.9631 loss_val: 27.1950 acc_val: 0.9637 time: 0.4019s\n",
      "7\n",
      "Epoch: 0140 loss_train: 27.2118 acc_train: 0.9631 loss_val: 27.1784 acc_val: 0.9637 time: 0.4478s\n",
      "21\n",
      "Epoch: 0141 loss_train: 27.1756 acc_train: 0.9630 loss_val: 27.1712 acc_val: 0.9636 time: 0.3969s\n",
      "6\n",
      "Epoch: 0142 loss_train: 27.1618 acc_train: 0.9631 loss_val: 27.1901 acc_val: 0.9636 time: 0.4179s\n",
      "4\n",
      "Epoch: 0143 loss_train: 27.1615 acc_train: 0.9631 loss_val: 27.1719 acc_val: 0.9637 time: 0.4318s\n",
      "3\n",
      "Epoch: 0144 loss_train: 27.1693 acc_train: 0.9631 loss_val: 27.1680 acc_val: 0.9636 time: 0.4109s\n",
      "4\n",
      "Epoch: 0145 loss_train: 27.1673 acc_train: 0.9631 loss_val: 27.1736 acc_val: 0.9637 time: 0.4149s\n",
      "22\n",
      "Epoch: 0146 loss_train: 27.1882 acc_train: 0.9630 loss_val: 27.1797 acc_val: 0.9637 time: 0.3830s\n",
      "32\n",
      "Epoch: 0147 loss_train: 27.1905 acc_train: 0.9629 loss_val: 27.1781 acc_val: 0.9636 time: 0.4159s\n",
      "21\n",
      "Epoch: 0148 loss_train: 27.1825 acc_train: 0.9630 loss_val: 27.1615 acc_val: 0.9637 time: 0.4019s\n",
      "25\n",
      "Epoch: 0149 loss_train: 27.2182 acc_train: 0.9630 loss_val: 27.1807 acc_val: 0.9637 time: 0.3959s\n",
      "0\n",
      "Epoch: 0150 loss_train: 27.1785 acc_train: 0.9631 loss_val: 27.1790 acc_val: 0.9636 time: 0.3999s\n",
      "38\n",
      "Epoch: 0151 loss_train: 27.1925 acc_train: 0.9629 loss_val: 27.1999 acc_val: 0.9635 time: 0.4328s\n",
      "14\n",
      "Epoch: 0152 loss_train: 27.1864 acc_train: 0.9630 loss_val: 27.1919 acc_val: 0.9637 time: 0.3870s\n",
      "11\n",
      "Epoch: 0153 loss_train: 27.1952 acc_train: 0.9631 loss_val: 27.1586 acc_val: 0.9637 time: 0.4229s\n",
      "10\n",
      "Epoch: 0154 loss_train: 27.1848 acc_train: 0.9631 loss_val: 27.2170 acc_val: 0.9636 time: 0.4169s\n",
      "9\n",
      "Epoch: 0155 loss_train: 27.1837 acc_train: 0.9631 loss_val: 27.1905 acc_val: 0.9637 time: 0.4308s\n",
      "11\n",
      "Epoch: 0156 loss_train: 27.2010 acc_train: 0.9631 loss_val: 27.1918 acc_val: 0.9637 time: 0.4338s\n",
      "6\n",
      "Epoch: 0157 loss_train: 27.1385 acc_train: 0.9631 loss_val: 27.1530 acc_val: 0.9636 time: 0.3989s\n",
      "13\n",
      "Epoch: 0158 loss_train: 27.1788 acc_train: 0.9630 loss_val: 27.1523 acc_val: 0.9635 time: 0.4099s\n",
      "3\n",
      "Epoch: 0159 loss_train: 27.1838 acc_train: 0.9631 loss_val: 27.1476 acc_val: 0.9636 time: 0.4219s\n",
      "8\n",
      "Epoch: 0160 loss_train: 27.1685 acc_train: 0.9631 loss_val: 27.1667 acc_val: 0.9636 time: 0.4149s\n",
      "16\n",
      "Epoch: 0161 loss_train: 27.1764 acc_train: 0.9630 loss_val: 27.1689 acc_val: 0.9636 time: 0.4114s\n",
      "18\n",
      "Epoch: 0162 loss_train: 27.1865 acc_train: 0.9630 loss_val: 27.1858 acc_val: 0.9637 time: 0.4239s\n",
      "10\n",
      "Epoch: 0163 loss_train: 27.1722 acc_train: 0.9631 loss_val: 27.1564 acc_val: 0.9637 time: 0.4259s\n",
      "33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0164 loss_train: 27.1752 acc_train: 0.9629 loss_val: 27.1629 acc_val: 0.9634 time: 0.4398s\n",
      "3\n",
      "Epoch: 0165 loss_train: 27.1674 acc_train: 0.9631 loss_val: 27.1483 acc_val: 0.9636 time: 0.5037s\n",
      "5\n",
      "Epoch: 0166 loss_train: 27.1669 acc_train: 0.9631 loss_val: 27.1642 acc_val: 0.9636 time: 0.4957s\n",
      "23\n",
      "Epoch: 0167 loss_train: 27.1517 acc_train: 0.9630 loss_val: 27.1566 acc_val: 0.9637 time: 0.4379s\n",
      "12\n",
      "Epoch: 0168 loss_train: 27.1656 acc_train: 0.9631 loss_val: 27.1497 acc_val: 0.9637 time: 0.4149s\n",
      "11\n",
      "Epoch: 0169 loss_train: 27.1330 acc_train: 0.9631 loss_val: 27.1175 acc_val: 0.9637 time: 0.4109s\n",
      "12\n",
      "Epoch: 0170 loss_train: 27.1383 acc_train: 0.9631 loss_val: 27.1629 acc_val: 0.9636 time: 0.4209s\n",
      "6\n",
      "Epoch: 0171 loss_train: 27.1268 acc_train: 0.9631 loss_val: 27.1285 acc_val: 0.9637 time: 0.4289s\n",
      "8\n",
      "Epoch: 0172 loss_train: 27.1852 acc_train: 0.9631 loss_val: 27.1518 acc_val: 0.9637 time: 0.4199s\n",
      "1\n",
      "Epoch: 0173 loss_train: 27.1531 acc_train: 0.9631 loss_val: 27.1588 acc_val: 0.9637 time: 0.4478s\n",
      "2\n",
      "Epoch: 0174 loss_train: 27.1762 acc_train: 0.9631 loss_val: 27.1670 acc_val: 0.9637 time: 0.4409s\n",
      "2\n",
      "Epoch: 0175 loss_train: 27.1397 acc_train: 0.9631 loss_val: 27.1342 acc_val: 0.9637 time: 0.4039s\n",
      "16\n",
      "Epoch: 0176 loss_train: 27.1614 acc_train: 0.9630 loss_val: 27.1420 acc_val: 0.9637 time: 0.4309s\n",
      "11\n",
      "Epoch: 0177 loss_train: 27.1690 acc_train: 0.9631 loss_val: 27.1793 acc_val: 0.9635 time: 0.3900s\n",
      "28\n",
      "Epoch: 0178 loss_train: 27.1443 acc_train: 0.9629 loss_val: 27.1328 acc_val: 0.9637 time: 0.4468s\n",
      "10\n",
      "Epoch: 0179 loss_train: 27.1829 acc_train: 0.9631 loss_val: 27.1440 acc_val: 0.9637 time: 0.4358s\n",
      "2\n",
      "Epoch: 0180 loss_train: 27.1575 acc_train: 0.9631 loss_val: 27.1445 acc_val: 0.9637 time: 0.4161s\n",
      "22\n",
      "Epoch: 0181 loss_train: 27.1404 acc_train: 0.9630 loss_val: 27.1299 acc_val: 0.9637 time: 0.4301s\n",
      "9\n",
      "Epoch: 0182 loss_train: 27.1472 acc_train: 0.9631 loss_val: 27.1468 acc_val: 0.9636 time: 0.4020s\n",
      "0\n",
      "Epoch: 0183 loss_train: 27.1131 acc_train: 0.9631 loss_val: 27.1192 acc_val: 0.9637 time: 0.4268s\n",
      "2\n",
      "Epoch: 0184 loss_train: 27.1472 acc_train: 0.9631 loss_val: 27.1635 acc_val: 0.9636 time: 0.4089s\n",
      "1\n",
      "Epoch: 0185 loss_train: 27.1711 acc_train: 0.9631 loss_val: 27.1519 acc_val: 0.9637 time: 0.4079s\n",
      "1\n",
      "Epoch: 0186 loss_train: 27.1344 acc_train: 0.9631 loss_val: 27.1434 acc_val: 0.9637 time: 0.4010s\n",
      "5\n",
      "Epoch: 0187 loss_train: 27.1304 acc_train: 0.9631 loss_val: 27.1246 acc_val: 0.9637 time: 0.4338s\n",
      "0\n",
      "Epoch: 0188 loss_train: 27.1450 acc_train: 0.9631 loss_val: 27.1404 acc_val: 0.9636 time: 0.4488s\n",
      "9\n",
      "Epoch: 0189 loss_train: 27.1557 acc_train: 0.9631 loss_val: 27.1463 acc_val: 0.9637 time: 0.4209s\n",
      "5\n",
      "Epoch: 0190 loss_train: 27.1540 acc_train: 0.9631 loss_val: 27.1646 acc_val: 0.9637 time: 0.3890s\n",
      "10\n",
      "Epoch: 0191 loss_train: 27.1681 acc_train: 0.9631 loss_val: 27.1514 acc_val: 0.9636 time: 0.4199s\n",
      "1\n",
      "Epoch: 0192 loss_train: 27.1275 acc_train: 0.9631 loss_val: 27.1244 acc_val: 0.9637 time: 0.4378s\n",
      "22\n",
      "Epoch: 0193 loss_train: 27.1658 acc_train: 0.9630 loss_val: 27.1833 acc_val: 0.9636 time: 0.4408s\n",
      "6\n",
      "Epoch: 0194 loss_train: 27.1368 acc_train: 0.9631 loss_val: 27.1367 acc_val: 0.9637 time: 0.4039s\n",
      "15\n",
      "Epoch: 0195 loss_train: 27.1579 acc_train: 0.9631 loss_val: 27.1382 acc_val: 0.9637 time: 0.4159s\n",
      "5\n",
      "Epoch: 0196 loss_train: 27.1244 acc_train: 0.9631 loss_val: 27.1190 acc_val: 0.9637 time: 0.4069s\n",
      "2\n",
      "Epoch: 0197 loss_train: 27.1341 acc_train: 0.9631 loss_val: 27.1214 acc_val: 0.9637 time: 0.4169s\n",
      "5\n",
      "Epoch: 0198 loss_train: 27.1340 acc_train: 0.9631 loss_val: 27.1178 acc_val: 0.9637 time: 0.4299s\n",
      "4\n",
      "Epoch: 0199 loss_train: 27.1454 acc_train: 0.9631 loss_val: 27.1579 acc_val: 0.9636 time: 0.4269s\n",
      "10\n",
      "Epoch: 0200 loss_train: 27.1239 acc_train: 0.9631 loss_val: 27.1201 acc_val: 0.9637 time: 0.4378s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 80.8307s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = np.sum([F.binary_cross_entropy_with_logits(output[idx_test][:,i], labels[idx_test][:,i]) for i in range(39)])\n",
    "    acc_test = accuracy_sample_class(threshold(output.detach().numpy()[idx_test]), labels.detach().numpy()[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 27.0957 accuracy= 0.9651\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with the imbalance of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider each label with its own binary classification problem, the dataset is clearly imbalanced in each problem with a favorable position for the label 0 that represents the absence of the label for the corresponding node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
